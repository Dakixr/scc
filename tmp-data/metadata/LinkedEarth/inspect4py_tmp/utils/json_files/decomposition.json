{"file": {"path": "/Users/dakixr/Desktop/github/scc/tmp-data/metadata/LinkedEarth/Pyleoclim_util/pyleoclim/utils/decomposition.py", "fileNameBase": "decomposition", "extension": "py", "doc": {"long_description": "@author: deborahkhider\nContains eigendecomposition methods:\nPrincipal Component Analysis, Singular Spectrum Analysis, Multi-channel SSA", "short_description": "Created on Tue Feb 25 06:29:36 2020", "full": "Created on Tue Feb 25 06:29:36 2020\n\n@author: deborahkhider\nContains eigendecomposition methods:\nPrincipal Component Analysis, Singular Spectrum Analysis, Multi-channel SSA"}}, "dependencies": [{"import": "numpy", "alias": "np", "type": "external"}, {"from_module": "statsmodels.multivariate.pca", "import": "PCA", "type": "external"}, {"from_module": "tsutils", "import": "standardize", "type": "external"}, {"from_module": "tsmodel", "import": "ar1_sim", "type": "external"}, {"from_module": "scipy.linalg", "import": "eigh", "type": "external"}, {"from_module": "scipy.linalg", "import": "toeplitz", "type": "external"}, {"from_module": "nitime", "import": "algorithms", "alias": "alg", "type": "external"}, {"import": "copy", "type": "external"}], "functions": {"mcpca": {"doc": {"long_description": "Carries out Principal Component Analysis  (most unfortunately named EOF analysis in the meteorology\nand climate literature) on a data matrix ys.\n\nThe significance of eigenvalues is gauged against those of AR(1) surrogates fit to the data.\n\nTODO: enable for ensembles and generally debug", "short_description": "Monte-Carlo Principal Component Analysis", "args": {"ys": {"description": "nt   = number of time samples (assumed identical for all records)\nnrec = number of records (aka variables, channels, etc)", "type_name": "2D numpy array (nt, nrec)", "is_optional": false}, "nMC": {"description": "the number of Monte-Carlo simulations", "type_name": "int", "is_optional": false}, "pca_kwargs": {"description": "keyword arguments for the PCA method", "type_name": "tuple", "is_optional": false}}, "returns": {"description": "- eigvals : eigenvalues (nrec,)\n\n- eigvals95 : eigenvalues of the AR(1) ensemble (nrec, nMC)\n\n- pcs : PC series of all components (nt, rec)\n\n- eofs : EOFs of all components (nrec, nrec)", "type_name": "dict containing:", "is_generator": false, "return_name": "res"}}, "args": ["ys", "nMC"], "returns": [["res"]], "min_max_lineno": {"min_lineno": 30, "max_lineno": 130}, "calls": ["min", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "statsmodels.multivariate.pca.PCA", "numpy.full", "range", "range", "numpy.percentile", "tsutils.eff_sample_size", "SpatialDecomp", "copy.deepcopy", "tsmodel.ar1_sim", "tsutils.eff_sample_size", "statsmodels.multivariate.pca.PCA", "numpy.sum"], "store_vars_calls": {"ncomp": "min", "pc_mc": "np.zeros", "eof_mc": "np.zeros", "eigvals": "np.zeros", "eig_ar1": "np.zeros", "pca_res": "PCA", "y_ar1": "np.full", "eig95": "np.percentile", "neff": "tsutils.eff_sample_size", "res": "SpatialDecomp", "yi": "copy.deepcopy", "pc_ar1": "PCA"}}, "pca_sklearn": {"doc": {"long_description": "Decomposition of a signal or data set in terms of orthogonal basis functions.\n\nFrom scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html", "short_description": "Principal Component Analysis (Empirical Orthogonal Functions)", "args": {"ys": {"description": "timeseries", "type_name": "array", "is_optional": false}, "n_components": {"description": "[default: None]\nNumber of components to keep. if n_components is not set all components are kept:\nIf n_components == 'mle' and svd_solver == 'full', Minka\u2019s MLE is used to guess the dimension. Use of n_components == 'mle' will interpret svd_solver == 'auto' as svd_solver == 'full'.\nIf 0 < n_components < 1 and svd_solver == 'full', select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components.\nIf svd_solver == 'arpack', the number of components must be strictly less than the minimum of n_features and n_samples.", "type_name": "int,None,or str", "is_optional": false, "default": "None"}, "copy": {"description": "[default: True]\nIf False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead.", "type_name": "bool,optional", "is_optional": false, "default": "True"}, "whiten": {"description": "[default: False]\nWhen True (False by default) the components_ vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.", "type_name": "bool,optional", "is_optional": false, "default": "False"}, "svd_solver": {"description": "If auto :\n    The solver is selected by a default policy based on X.shape and n_components: if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient \u2018randomized\u2019 method is enabled.\n    Otherwise the exact full SVD is computed and optionally truncated afterwards.\n\nIf full :\n    run exact full SVD calling the standard LAPACK solver via scipy.linalg.svd and select the components by postprocessing\n\nIf arpack :\n    run SVD truncated to n_components calling ARPACK solver via scipy.sparse.linalg.svds. It requires strictly 0 < n_components < min(X.shape)\n\nIf randomized :\n    run randomized SVD by the method of Halko et al.", "type_name": "str {\u2018auto\u2019, \u2018full\u2019, \u2018arpack\u2019, \u2018randomized\u2019}", "is_optional": false, "default": "policy"}, "tol": {"description": "[default: 0]\nTolerance for singular values computed by svd_solver == \u2018arpack\u2019.", "type_name": "float >= 0 ,optional", "is_optional": false, "default": "0"}, "iterated_power": {"description": "[default: 'auto']\nNumber of iterations for the power method computed by svd_solver == \u2018randomized\u2019.", "type_name": "int >= 0, or string {'auto'}", "is_optional": false}, "random_state": {"description": "[default: None]\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator;\nIf None, the random number generator is the RandomState instance used by np.random.\nUsed when svd_solver == \u2018arpack\u2019 or \u2018randomized\u2019.", "type_name": "int, RandomState instance, or None", "is_optional": true, "default": "None"}}, "returns": {"description": "Sklearn PCA object dictionary of all attributes and values.\n\n-components_array, shape (n_components, n_features)\n    Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by explained_variance_.\n\n-explained_variance_array, shape (n_components,)\n    The amount of variance explained by each of the selected components.\n    Equal to n_components largest eigenvalues of the covariance matrix of X.\n    New in version 0.18.\n\n-explained_variance_ratio_array, shape (n_components,)\n    Percentage of variance explained by each of the selected components.\n    If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.\n\n-singular_values_array, shape (n_components,)\n    The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the n_components variables in the lower-dimensional space.\n    New in version 0.19.\n\n-mean_array, shape (n_features,)\n    Per-feature empirical mean, estimated from the training set.\n    Equal to X.mean(axis=0).\n\n-n_components_int\n    The estimated number of components. When n_components is set to \u2018mle\u2019 or a number between 0 and 1 (with svd_solver == \u2018full\u2019) this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None.\n\n-n_features_int\n    Number of features in the training data.\n\n-n_samples_int\n    Number of samples in the training data.\n\n-noise_variance_float\n    The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See \u201cPattern Recognition and Machine Learning\u201d by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf. It is required to compute the estimated data covariance and score samples.\n    Equal to the average of (min(n_features, n_samples) - n_components) smallest eigenvalues of the covariance matrix of X.", "type_name": "dict", "is_generator": false}}, "args": ["ys", "n_components", "copy", "whiten", "svd_solver", "tol", "iterated_power", "random_state"], "min_max_lineno": {"min_lineno": 134, "max_lineno": 233}, "calls": ["numpy.any", "statsmodels.multivariate.pca.PCA", "numpy.isnan", "ValueError", "statsmodels.multivariate.pca.PCA.fit"], "store_vars_calls": {"pca": "PCA"}}, "mssa": {"doc": {"long_description": "Multivariate generalization of SSA [2], using the original algorithm of [1].\nEach variable is called a channel, hence the name.", "short_description": "Multi-channel singular spectrum analysis analysis", "args": {"ys": {"description": "multiple time series (dimension: length of time series x total number of time series)", "type_name": "array", "is_optional": false}, "M": {"description": "window size (embedding dimension, default: 10% of the length of the series)", "type_name": "int", "is_optional": false, "default": "10"}, "nMC": {"description": "Number of iteration in the Monte-Carlo process [default=0, no Monte Carlo process]", "type_name": "int", "is_optional": false}, "f": {"description": "fraction (0<f<=1) of good data points for identifying significant PCs [f = 0.3]", "type_name": "float", "is_optional": false}}, "returns": {"description": "Containing:\n\n- eigvals : array of eigenvalue spectrum\n\n- eigvals05 : The 5% percentile of eigenvalues\n\n- eigvals95 : The 95% percentile of eigenvalues\n\n- PC : matrix of principal components (2D array)\n\n- RC : matrix of RCs (nrec,N,nrec*M) (2D array)", "type_name": "dict", "is_generator": false, "return_name": "res"}}, "args": ["ys", "M", "nMC", "f"], "returns": [["res"]], "min_max_lineno": {"min_lineno": 235, "max_lineno": 361}, "calls": ["len", "len", "numpy.zeros", "numpy.arange", "scipy.linalg.eigh", "numpy.sort", "numpy.argsort", "numpy.zeros", "numpy.zeros", "numpy.arange", "numpy.zeros", "numpy.arange", "range", "numpy.percentile", "numpy.percentile", "numpy.zeros", "numpy.arange", "numpy.zeros", "numpy.arange", "int", "numpy.arange", "numpy.dot", "nitime.algorithms.AR_est_YW", "numpy.sqrt", "range", "numpy.arange", "numpy.diag", "numpy.arange", "numpy.arange", "numpy.nan_to_num", "numpy.nan_to_num", "numpy.arange", "numpy.dot", "numpy.dot", "sum", "numpy.dot", "numpy.flipud", "numpy.arange", "numpy.transpose", "numpy.std", "numpy.nan_to_num", "numpy.nan_to_num", "numpy.dot", "sum", "numpy.expand_dims", "numpy.expand_dims", "numpy.diagonal().mean", "numpy.isnan", "numpy.random.randn", "numpy.mean", "numpy.transpose", "numpy.transpose", "numpy.isnan", "numpy.diagonal", "numpy.isnan"], "store_vars_calls": {"N": "len", "nrec": "len", "Y": "np.zeros", "sort_tmp": "np.sort", "sortarg": "np.argsort", "Ym": "np.zeros", "noise": "np.zeros", "eigvals_R": "np.zeros", "eigvals95": "np.percentile", "eigvals05": "np.percentile", "PC": "np.zeros", "RC": "np.zeros", "M": "int", "sigma_est": "np.sqrt", "ngood": "sum", "x2": "np.flipud"}}, "ssa": {"doc": {"long_description": "Nonparametric eigendecomposition of timeseries into orthogonal oscillations.\nThis implementation  uses the method of [1], with applications presented in [2].\nOptionally (nMC>0), the significance of eigenvalues is assessed by Monte-Carlo simulations of an AR(1) model fit to X, using [3].\nThe method expects regular spacing, but is tolerant to missing values, up to a fraction 0<f<1 (see [4]).", "short_description": "Singular spectrum analysis", "args": {"y": {"description": "time series (evenly-spaced, possibly with up to f*N NaNs)", "type_name": "array of length N", "is_optional": false}, "M": {"description": "window size (default: 10% of N)", "type_name": "int", "is_optional": false, "default": "10"}, "nMC": {"description": "Number of iterations in the Monte-Carlo simulation (default nMC=0, bypasses Monte Carlo SSA)\nCurrently only supported for evenly-spaced, gap-free data.", "type_name": "int", "is_optional": false, "default": "nMC"}, "f": {"description": "maximum allowable fraction of missing values. (Default is 0.5)", "type_name": "float", "is_optional": false, "default": "0.5"}, "trunc": {"description": "if present, truncates the expansion to a level K < M owing to one of 3 criteria:\n    (1) 'kaiser': variant of the Kaiser-Guttman rule, retaining eigenvalues larger than the median\n    (2) 'mcssa': Monte-Carlo SSA (use modes above the 95% quantile from an AR(1) process)\n    (3) 'var': first K modes that explain at least var_thresh % of the variance.\nDefault is None, which bypasses truncation (K = M)", "type_name": "str", "is_optional": false, "default": "None"}, "var_thresh": {"description": "variance threshold for reconstruction (only impactful if trunc is set to 'var')", "type_name": "float", "is_optional": false}}, "returns": {"description": "- eigvals : (M, ) array of eigenvalues\n\n- eigvecs : (M, M) Matrix of temporal eigenvectors (T-EOFs)\n\n- PC : (N - M + 1, M) array of principal components (T-PCs)\n\n- RCmat : (N,  M) array of reconstructed components\n\n- RCseries : (N,) reconstructed series, with mean and variance restored\n\n- pctvar: (M, ) array of the fraction of variance (%) associated with each mode\n\n- eigvals_q : (M, 2) array contaitning the 5% and 95% quantiles of the Monte-Carlo eigenvalue spectrum [ if nMC >0 ]", "type_name": "dict containing:", "is_generator": false, "return_name": "res"}}, "args": ["y", "M", "nMC", "f", "trunc", "var_thresh"], "returns": [["res"]], "min_max_lineno": {"min_lineno": 362, "max_lineno": 534}, "calls": ["tsutils.standardize", "len", "numpy.zeros", "range", "scipy.linalg.toeplitz", "scipy.linalg.eigh", "numpy.sort", "numpy.argsort", "numpy.zeros", "numpy.arange", "len", "numpy.zeros", "range", "RCmat[].sum", "int", "numpy.arange", "tsmodel.ar1_sim", "numpy.zeros", "numpy.arange", "range", "numpy.empty", "numpy.percentile", "numpy.percentile", "numpy.arange", "ValueError", "numpy.dot", "numpy.flipud", "numpy.arange", "numpy.tile", "sum", "sum", "numpy.sum", "tsutils.standardize", "numpy.correlate", "scipy.linalg.toeplitz", "numpy.diag", "numpy.where", "numpy.median", "numpy.expand_dims", "numpy.expand_dims", "numpy.diagonal().mean", "sum", "numpy.dot", "numpy.where", "numpy.arange", "numpy.where", "numpy.isnan", "abs", "numpy.dot", "numpy.diagonal", "numpy.isnan", "numpy.isnan", "sum", "numpy.transpose", "numpy.argwhere", "numpy.isnan", "numpy.cumsum"], "store_vars_calls": {"N": "len", "c": "np.zeros", "C": "toeplitz", "sort_tmp": "np.sort", "sortarg": "np.argsort", "PC": "np.zeros", "nmodes": "len", "RCmat": "np.zeros", "RCseries": "RCmat[].sum", "M": "int", "noise": "ar1_sim", "eigvals_R": "np.zeros", "lgs": "np.arange", "eigvals_q": "np.empty", "mode_idx": "np.arange", "xdum": "np.flipud", "ngood": "sum", "Gn": "np.correlate", "Cn": "toeplitz", "mval": "np.median"}}}, "is_test": false}