{"file": {"path": "/Users/dakixr/Desktop/github/scc/tmp-data/metadata/LinkedEarth/Pyleoclim_util/pyleoclim/utils/correlation.py", "fileNameBase": "correlation", "extension": "py", "doc": {"long_description": "@author: deborahkhider\n\nContains all relevant functions for correlation analysis", "short_description": "Created on Tue Feb 25 06:01:55 2020", "full": "Created on Tue Feb 25 06:01:55 2020\n\n@author: deborahkhider\n\nContains all relevant functions for correlation analysis"}}, "dependencies": [{"import": "numpy", "alias": "np", "type": "external"}, {"from_module": "scipy.stats", "import": "pearsonr", "type": "external"}, {"from_module": "scipy.stats.mstats", "import": "gmean", "type": "external"}, {"from_module": "scipy.stats", "import": "t", "alias": "stu", "type": "external"}, {"from_module": "scipy.stats", "import": "gaussian_kde", "type": "external"}, {"import": "statsmodels", "alias": "sm", "type": "external"}, {"from_module": "sklearn", "import": "preprocessing", "type": "external"}, {"from_module": "tsmodel", "import": "ar1_fit_evenly", "type": "external"}], "functions": {"corr_sig": {"doc": {"long_description": "The significance of the correlation is assessed using one of the following methods:\n    \n1) 'ttest': T-test adjusted for effective sample size. \n    This is a parametric test (data are Gaussian and identically distributed) with a rather ad-hoc adjustment. \n    It is instantaneous but makes a lot of assumptions about the data, many of which may not be met.\n2) 'isopersistent': AR(1) modeling of x and y.\n    This is a parametric test as well (series follow an AR(1) model) but \n    solves the issue by direct simulation. \n3) 'isospectral': phase randomization of original inputs. (default)\n    This is a non-parametric method, assuming only wide-sense stationarity.\n    \n\nFor 2 and 3, computational requirements scale with nsim.\nWhen possible, nsim should be at least 1000.", "short_description": "Estimates the Pearson's correlation and associated significance between two non IID time series", "args": {"y1": {"description": "vector of (real) numbers of same length as y2, no NaNs allowed", "type_name": "array", "is_optional": false}, "y2": {"description": "vector of (real) numbers of same length as y1, no NaNs allowed", "type_name": "array", "is_optional": false}, "nsim": {"description": "the number of simulations [default: 1000]", "type_name": "int", "is_optional": false, "default": "1000"}, "method": {"description": "method for significance testing", "type_name": "{'ttest','isopersistent','isospectral' (default)}", "is_optional": false}, "alpha": {"description": "significance level for critical value estimation [default: 0.05]", "type_name": "float", "is_optional": false, "default": "0.05"}}, "returns": {"description": "the result dictionary, containing\n\n- r : float\n    correlation coefficient\n- p : float \n    the p-value\n- signif : bool\n    true if significant; false otherwise\n    Note that signif = True if and only if p <= alpha.", "type_name": "dict ", "is_generator": false, "return_name": "res"}}, "args": ["y1", "y2", "nsim", "method", "alpha"], "returns": [["res"]], "min_max_lineno": {"min_lineno": 26, "max_lineno": 100}, "calls": ["numpy.array", "numpy.array", "numpy.size", "numpy.size", "correlation.corr_ttest", "correlation.corr_isopersist", "correlation.corr_isospec"], "store_vars_calls": {"y1": "np.array", "y2": "np.array"}}, "fdr": {"doc": {"long_description": "The false discovery rate is a method of conceptualizing the rate of type I errors in null hypothesis testing when conducting multiple comparisons. \nTranslated from fdr.R by Dr. Chris Paciorek", "short_description": "Determine significance based on the FDR approach", "args": {"pvals": {"description": "A vector of p-values on which to conduct the multiple testing.", "type_name": "list or array", "is_optional": false}, "qlevel": {"description": "The proportion of false positives desired.", "type_name": "float", "is_optional": false}, "method": {"description": "Method for performing the testing.\n    - 'original' follows Benjamini & Hochberg (1995);\n    - 'general' is much more conservative, requiring no assumptions on the p-values (see Benjamini & Yekutieli (2001)).\n    'original' is recommended, and if desired, using 'adj_method=\"mean\"' to increase power.", "type_name": "{'original', 'general'}", "is_optional": false}, "adj_method": {"description": "Method for increasing the power of the procedure by estimating the proportion of alternative p-values.\n    - 'mean', the modified Storey estimator in Ventura et al. (2004)\n    - 'storey', the method of Storey (2002)\n    - 'two-stage', the iterative approach of Benjamini et al. (2001)", "type_name": "{'mean', 'storey', 'two-stage'}", "is_optional": false}, "adj_args": {"description": "Arguments for adj_method; see prop_alt() for description,\nbut note that for \"two-stage\", qlevel and fdr_method are taken from the qlevel and method arguments for fdr()", "type_name": "dict", "is_optional": false}}, "returns": {"description": "A vector of the indices of the significant tests; None if no significant tests", "type_name": "array or None", "is_generator": false, "return_name": "fdr_res"}}, "args": ["pvals", "qlevel", "method", "adj_method", "adj_args"], "returns": [["fdr_res"]], "min_max_lineno": {"min_lineno": 101, "max_lineno": 171}, "calls": ["len", "correlation.prop_alt", "numpy.arange", "correlation.fdr_master", "print", "print"], "store_vars_calls": {"n": "len", "a": "prop_alt", "fdr_res": "fdr_master"}}, "corr_ttest": {"doc": {"long_description": "the classical T-test adjusted for effective sample size.\n\nThe degrees of freedom are adjusted following n_eff=n(1-g)/(1+g) where g is the lag-1 autocorrelation.", "short_description": "Estimates the significance of correlations between 2 time series using", "args": {"y1": {"description": "vectors of (real) numbers with identical length, no NaNs allowed", "type_name": "array", "is_optional": false}, "y2": {"description": "vectors of (real) numbers with identical length, no NaNs allowed", "type_name": "array", "is_optional": false}, "alpha": {"description": "significance level for critical value estimation [default: 0.05]", "type_name": "float", "is_optional": false, "default": "0.05"}}, "returns": {"description": "correlation between x and y", "type_name": "float", "is_generator": false, "return_name": "r"}}, "args": ["y1", "y2", "alpha"], "returns": [["r", "signif", "pval"]], "min_max_lineno": {"min_lineno": 176, "max_lineno": 236}, "calls": ["tsmodel.ar1_fit_evenly", "tsmodel.ar1_fit_evenly", "numpy.size", "scipy.stats.mstats.gmean", "scipy.stats.pearsonr", "numpy.abs", "numpy.sqrt", "scipy.stats.t.cdf", "numpy.abs"], "store_vars_calls": {"g1": "ar1_fit_evenly", "g2": "ar1_fit_evenly", "N": "np.size", "Ne": "gmean"}}, "corr_isopersist": {"doc": {"long_description": "The significance is gauged via a non-parametric (Monte Carlo) simulation of\ncorrelations with nsim AR(1) processes with identical persistence\nproperties as x and y ; the measure of which is the lag-1 autocorrelation (g).", "short_description": "Computes the Pearson's correlation between two timeseries, and their significance using Ar(1) modeling.", "args": {"y1": {"description": "vectors of (real) numbers with identical length, no NaNs allowed", "type_name": "array", "is_optional": false}, "y2": {"description": "vectors of (real) numbers with identical length, no NaNs allowed", "type_name": "array", "is_optional": false}, "alpha": {"description": "significance level for critical value estimation [default: 0.05]", "type_name": "float", "is_optional": false, "default": "0.05"}, "nsim": {"description": "number of simulations [default: 1000]", "type_name": "int", "is_optional": false, "default": "1000"}}, "returns": {"description": "correlation between x and y", "type_name": "float", "is_generator": false, "return_name": "r"}}, "args": ["y1", "y2", "alpha", "nsim"], "returns": [["r", "signif", "pval"]], "min_max_lineno": {"min_lineno": 237, "max_lineno": 316}, "calls": ["numpy.abs", "correlation.isopersistent_rn", "correlation.isopersistent_rn", "numpy.zeros", "numpy.arange", "numpy.abs", "numpy.linspace", "scipy.stats.gaussian_kde", "numpy.abs", "numpy.argmin", "numpy.trapz", "numpy.percentile", "scipy.stats.pearsonr", "scipy.stats.gaussian_kde.", "scipy.stats.pearsonr", "numpy.max", "numpy.max"], "store_vars_calls": {"ra": "np.abs", "rs": "np.zeros", "rsa": "np.abs", "xi": "np.linspace", "kde": "gaussian_kde", "diff": "np.abs", "pos": "np.argmin", "pval": "np.trapz", "rcrit": "np.percentile"}}, "isopersistent_rn": {"doc": {"long_description": "with same persistence properties as X (Mean and variance are also preserved).", "short_description": "Generates p realization of a red noise [i.e. AR(1)] process", "args": {"X": {"description": "vector of (real) numbers as a time series, no NaNs allowed", "type_name": "array", "is_optional": false}, "p": {"description": "number of simulations", "type_name": "int", "is_optional": false}}, "returns": {"description": "n rows by p columns matrix of an AR1 process, where n is the size of X", "type_name": "numpy array", "is_generator": false, "return_name": "red"}}, "args": ["X", "p"], "returns": [["red", "g"]], "min_max_lineno": {"min_lineno": 317, "max_lineno": 351}, "calls": ["numpy.size", "numpy.std", "tsmodel.ar1_fit_evenly", "correlation.sm_ar1_sim"], "store_vars_calls": {"n": "np.size", "sig": "np.std", "g": "ar1_fit_evenly", "red": "sm_ar1_sim"}}, "sm_ar1_sim": {"doc": {"short_description": "Produce p realizations of an AR1 process of length n with lag-1 autocorrelation g using statsmodels", "args": {"n": {"description": "row dimensions", "type_name": "int", "is_optional": false}, "p": {"description": "column dimensions", "type_name": "int", "is_optional": false}, "g": {"description": "lag-1 autocorrelation coefficient", "type_name": "float", "is_optional": false}, "sig": {"description": "the standard deviation of the original time series", "type_name": "float", "is_optional": false}}, "returns": {"description": "n rows by p columns matrix of an AR1 process", "type_name": "numpy matrix", "is_generator": false, "return_name": "red"}}, "args": ["n", "p", "g", "sig"], "returns": [["red"]], "min_max_lineno": {"min_lineno": 353, "max_lineno": 388}, "calls": ["numpy.empty", "numpy.arange", "numpy.sqrt", "statsmodels.tsa.arma_generate_sample"], "store_vars_calls": {"red": "np.empty"}}, "red_noise": {"doc": {"short_description": "Produce M realizations of an AR1 process of length N with lag-1 autocorrelation g", "args": {"N": {"description": "row dimensions", "type_name": "int", "is_optional": false}, "M": {"description": "column dimensions", "type_name": "int", "is_optional": false}, "g": {"description": "lag-1 autocorrelation coefficient", "type_name": "float", "is_optional": false}}, "returns": {"description": "N rows by M columns matrix of an AR1 process", "type_name": "numpy array", "is_generator": false, "return_name": "red"}}, "args": ["N", "M", "g"], "returns": [["red"]], "min_max_lineno": {"min_lineno": 389, "max_lineno": 422}, "calls": ["numpy.zeros", "numpy.random.randn", "numpy.arange", "numpy.random.randn"], "store_vars_calls": {"red": "np.zeros"}}, "corr_isospec": {"doc": {"long_description": "Estimates the significance of correlations between non IID\ntime series by phase randomization of original inputs.\nThis function creates 'nsim' random time series that have the same power\nspectrum as the original time series but random phases.", "short_description": "Estimates the significance of the correlation using phase randomization", "args": {"y1": {"description": "vectors of (real) numbers with identical length, no NaNs allowed", "type_name": "array", "is_optional": false}, "y2": {"description": "vectors of (real) numbers with identical length, no NaNs allowed", "type_name": "array", "is_optional": false}, "alpha": {"description": "significance level for critical value estimation [default: 0.05]", "type_name": "float", "is_optional": false, "default": "0.05"}, "nsim": {"description": "number of simulations [default: 1000]", "type_name": "int", "is_optional": false, "default": "1000"}}, "returns": {"description": "correlation between y1 and y2", "type_name": "float", "is_generator": false, "return_name": "r"}}, "args": ["y1", "y2", "alpha", "nsim"], "returns": [["r", "signif", "F"]], "min_max_lineno": {"min_lineno": 423, "max_lineno": 499}, "calls": ["correlation.phaseran", "correlation.phaseran", "sklearn.preprocessing.scale", "sklearn.preprocessing.scale", "numpy.size", "numpy.diag", "scipy.stats.pearsonr", "numpy.dot", "numpy.sum", "numpy.transpose", "numpy.abs", "numpy.abs"], "store_vars_calls": {"Y1surr": "phaseran", "Y2surr": "phaseran", "Y1s": "preprocessing.scale", "Y2s": "preprocessing.scale", "n": "np.size", "rSim": "np.diag"}}, "phaseran": {"doc": {"long_description": "It creates blocks of surrogate data with the same second order properties as the original\ntime series dataset by transforming the oriinal data into the frequency domain, randomizing the\nphases simultaneoulsy across the time series and converting the data back into the time domain. \n\nWritten by Carlos Gias for MATLAB\n\nhttp://www.mathworks.nl/matlabcentral/fileexchange/32621-phase-randomization/content/phaseran.m", "short_description": "Simultaneous phase randomization of a set of time series", "args": {"recblk": {"description": "2D array , Row: time sample. Column: recording.\nAn odd number of time samples (height) is expected.\nIf that is not the case, recblock is reduced by 1 sample before the surrogate data is created.\nThe class must be double and it must be nonsparse.", "type_name": "numpy array", "is_optional": false}, "nsurr": {"description": "is the number of image block surrogates that you want to generate.", "type_name": "int", "is_optional": false}}, "returns": {"description": "3D multidimensional array image block with the surrogate datasets along the third dimension", "type_name": "numpy array", "is_generator": false, "return_name": "surrblk"}}, "args": ["recblk", "nsurr"], "returns": [["surrblk"]], "min_max_lineno": {"min_lineno": 500, "max_lineno": 570}, "calls": ["int", "numpy.arange", "numpy.arange", "numpy.fft.fft", "numpy.zeros", "numpy.arange", "numpy.random.rand", "numpy.exp", "numpy.conj", "numpy.copy", "numpy.real", "numpy.flipud", "numpy.fft.ifft"], "store_vars_calls": {"len_ser": "int", "interv1": "np.arange", "interv2": "np.arange", "fft_recblk": "np.fft.fft", "surrblk": "np.zeros", "ph_rnd": "np.random.rand", "ph_interv1": "np.exp", "ph_interv2": "np.conj", "fft_recblk_surr": "np.copy"}}, "fdr_basic": {"doc": {"short_description": "The basic FDR of Benjamini & Hochberg (1995).", "args": {"pvals": {"description": "A vector of p-values on which to conduct the multiple testing.", "type_name": "list or array", "is_optional": false}, "qlevel": {"description": "The proportion of false positives desired.", "type_name": "float", "is_optional": false}}, "returns": {"description": "A vector of the indices of the significant tests; None if no significant tests", "type_name": "array or None", "is_generator": false, "return_name": "fdr_res"}}, "args": ["pvals", "qlevel"], "returns": [["fdr_res"]], "min_max_lineno": {"min_lineno": 573, "max_lineno": 612}, "calls": ["len", "numpy.sort", "numpy.argsort", "numpy.max", "numpy.arange", "numpy.arange", "numpy.sort", "numpy.arange"], "store_vars_calls": {"n": "len", "sorted_pvals": "np.sort", "sort_index": "np.argsort", "num_reject": "np.max", "indices": "np.arange", "fdr_res": "np.sort"}}, "fdr_master": {"doc": {"short_description": "Perform various versions of the FDR procedure", "args": {"pvals": {"description": "A vector of p-values on which to conduct the multiple testing.", "type_name": "list or array", "is_optional": false}, "qlevel": {"description": "The proportion of false positives desired.", "type_name": "float", "is_optional": false}, "method": {"description": "Method for performing the testing.\n- 'original' follows Benjamini & Hochberg (1995);\n- 'general' is much more conservative, requiring no assumptions on the p-values (see Benjamini & Yekutieli (2001)).\nWe recommend using 'original', and if desired, using 'adj_method=\"mean\"' to increase power.", "type_name": "{'original', 'general'}", "is_optional": false}}, "returns": {"description": "A vector of the indices of the significant tests; None if no significant tests", "type_name": "array or None", "is_generator": false, "return_name": "fdr_res"}}, "args": ["pvals", "qlevel", "method"], "returns": [["fdr_res"]], "min_max_lineno": {"min_lineno": 613, "max_lineno": 651}, "calls": ["correlation.fdr_basic", "len", "numpy.sum", "numpy.arange"], "store_vars_calls": {"fdr_res": "fdr_basic", "n": "len"}}, "storey": {"doc": {"short_description": "The basic Storey (2002) estimator of a, the proportion of alternative hypotheses.", "args": {"edf_quantile": {"description": "The quantile of the empirical distribution function at which to estimate a.", "type_name": "float", "is_optional": false}, "pvals": {"description": "A vector of p-values on which to estimate a", "type_name": "list or array", "is_optional": false}}, "returns": {"description": "estimate of a, the number of alternative hypotheses", "type_name": "int", "is_generator": false, "return_name": "a"}}, "args": ["edf_quantile", "pvals"], "returns": [["a"]], "min_max_lineno": {"min_lineno": 652, "max_lineno": 683}, "calls": ["numpy.array", "numpy.max", "ValueError", "numpy.mean"], "store_vars_calls": {"pvals": "np.array", "a": "np.max"}}, "prop_alt": {"doc": {"short_description": "Calculate an estimate of a, the proportion of alternative hypotheses, using one of several methods", "args": {"pvals": {"description": "A vector of p-values on which to estimate a", "type_name": "list or array", "is_optional": false}, "adj_method": {"description": "Method for increasing the power of the procedure by estimating the proportion of alternative p-values.\n- 'mean', the modified Storey estimator that we suggest in Ventura et al. (2004)\n- 'storey', the method of Storey (2002)\n- 'two-stage', the iterative approach of Benjamini et al. (2001)", "type_name": "{'mean', 'storey', 'two-stage'}", "is_optional": false}, "adj_args": {"description": "- for \"mean\", specify \"edf_lower\", the smallest quantile at which to estimate a, and \"num_steps\", the number of quantiles to use\n  the approach uses the average of the Storey (2002) estimator for the num_steps quantiles starting at \"edf_lower\" and finishing just less than 1\n- for \"storey\", specify \"edf_quantile\", the quantile at which to calculate the estimator\n- for \"two-stage\", the method uses a standard FDR approach to estimate which p-values are significant\n  this number is the estimate of a; therefore the method requires specification of qlevel,\n  the proportion of false positives and \"fdr_method\" ('original' or 'general'), the FDR method to be used.\n  We do not recommend 'general' as this is very conservative and will underestimate a.", "type_name": "dict", "is_optional": false}}, "returns": {"description": "estimate of a, the number of alternative hypotheses", "type_name": "int", "is_generator": false, "return_name": "a"}}, "args": ["pvals", "adj_method", "adj_args"], "returns": [["a"], ["a"], ["a"]], "min_max_lineno": {"min_lineno": 684, "max_lineno": 754}, "calls": ["len", "correlation.fdr_master", "len", "correlation.storey", "ValueError", "numpy.linspace", "numpy.mean", "ValueError", "ValueError", "ValueError", "correlation.storey", "type"], "store_vars_calls": {"n": "len", "fdr_res": "fdr_master", "a": "np.mean", "edf_quantiles": "np.linspace"}}}, "is_test": true}