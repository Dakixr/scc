{"file": {"path": "/Users/dakixr/Desktop/github/scc/tmp-data/metadata/KnowledgeCaptureAndDiscoveryANDmintproject/P4ML-UI/spider/spider/base.py", "fileNameBase": "base", "extension": "py"}, "dependencies": [{"from_module": "__future__", "import": "absolute_import", "type": "external"}, {"from_module": "__future__", "import": "division", "type": "external"}, {"from_module": "__future__", "import": "print_function", "type": "external"}, {"from_module": "__future__", "import": "unicode_literals", "type": "external"}, {"from_module": "six", "import": "with_metaclass", "alias": "_py_backwards_six_withmetaclass", "type": "external"}, {"import": "abc", "type": "external"}, {"import": "random", "type": "external"}, {"import": "sys", "type": "external"}], "classes": {"PrimitiveBase": {"doc": {"long_description": "Class is parametrized using three type variables, ``Inputs``, ``Outputs``, and ``Params``.\n``Params`` has to be a subclass of a `NamedTuple` and subclasses of this class should\ndefine types for all fields of a provided named tuple.`\n\nAll arguments to all methods are keyword-only. In Python 3 this is enforced, in Python 2\nthis is not, but callers should still use only keyword-based arguments when calling to\nbe backwards and future compatible.\n\nMethods not part of standardized interface classes should be seen as private.\nStandardized interface does not use attributes so all attributes on classes are seen\nas private as well. Consider using the convention that private symbols should start with ``_``.\n\nMethod arguments which start with ``_`` are seen as private and can be used for arguments\nuseful for debugging and testing, but they should not be used by (or even known to) a\ncaller during normal execution. Such arguments have to be optional (have a default value)\nso that the method can be called without the knowledge of the argument.\n\nSubclasses of this class allow functional compositionality.", "short_description": "A base class for all TA1 primitives.", "full": "A base class for all TA1 primitives.\n\nClass is parametrized using three type variables, ``Inputs``, ``Outputs``, and ``Params``.\n``Params`` has to be a subclass of a `NamedTuple` and subclasses of this class should\ndefine types for all fields of a provided named tuple.`\n\nAll arguments to all methods are keyword-only. In Python 3 this is enforced, in Python 2\nthis is not, but callers should still use only keyword-based arguments when calling to\nbe backwards and future compatible.\n\nMethods not part of standardized interface classes should be seen as private.\nStandardized interface does not use attributes so all attributes on classes are seen\nas private as well. Consider using the convention that private symbols should start with ``_``.\n\nMethod arguments which start with ``_`` are seen as private and can be used for arguments\nuseful for debugging and testing, but they should not be used by (or even known to) a\ncaller during normal execution. Such arguments have to be optional (have a default value)\nso that the method can be called without the knowledge of the argument.\n\nSubclasses of this class allow functional compositionality."}, "min_max_lineno": {"min_lineno": 31, "max_lineno": 73}, "methods": {"__init__": {"doc": {"long_description": "level in their ``__init__`` as explicit typed keyword-only arguments\n(no ``*args`` or ``**kwargs``).\n\nHyper-parameters are those primitive's parameters which are not changing during\na life-time of a primitive. Parameters which do are set using the ``set_params`` method.", "short_description": "All primitives should specify all the hyper-parameters that can be set at the class"}, "args": ["self"], "min_max_lineno": {"min_lineno": 35, "max_lineno": 38}}, "produce": {"doc": {"long_description": "In many cases producing an output is a quick operation in comparison with ``fit``, but not\nall cases are like that. For example, a primitive can start a potentially long optimization\nprocess to compute outputs. ``timeout`` and ``iterations`` can serve as a way for a caller\nto guide the length of this process.\n\nIdeally, a primitive should adapt its call to try to produce the best outputs possible\ninside the time allocated. If this is not possible and the primitive reaches the timeout\nbefore producing outputs, it should raise a ``TimeoutError`` exception to signal that the\ncall was unsuccessful in the given time. The state of the primitive after the exception\nshould be as the method call has never happened and primitive should continue to operate\nnormally. The purpose of ``timeout`` is to give opportunity to a primitive to cleanly\nmanage its state instead of interrupting execution from outside. Maintaining stable internal\nstate should have precedence over respecting the ``timeout`` (caller can terminate the\nmisbehaving primitive from outside anyway). If a longer ``timeout`` would produce\ndifferent outputs, then ``get_call_metadata``'s ``has_finished`` should be set to\n``False``.\n\nSome primitives have internal iterations (for example, optimization iterations).\nFor those, caller can provide how many of primitive's internal iterations\nshould a primitive do before returning outputs. Primitives should make iterations as\nsmall as reasonable. If ``iterations`` is ``None``, then there is no limit on\nhow many iterations the primitive should do and primitive should choose the best amount\nof iterations on its own (potentially controlled through hyper-parameters).\nIf ``iterations`` is a number, a primitive has to do those number of iterations,\nif possible. ``timeout`` should still be respected and potentially less iterations\ncan be done because of that. Primitives with internal iterations should make\n``get_call_metadata`` returns correct values.\n\nFor primitives which do not have internal iterations, any value of ``iterations``\nmeans that they should run fully, respecting only ``timeout``.", "short_description": "Produce primitive's best choice of the output for each of the inputs.", "args": {"inputs": {"description": "The inputs of shape [num_inputs, ...].", "type_name": "Inputs", "is_optional": false}, "timeout": {"description": "A maximum time this primitive should take to produce outputs during this method call, in seconds.", "type_name": "float", "is_optional": false}, "iterations": {"description": "How many of internal iterations should the primitive do.", "type_name": "int", "is_optional": false}}, "returns": {"description": "The outputs of shape [num_inputs, ...].", "type_name": "Outputs", "is_generator": false}}, "args": ["self", "inputs", "timeout", "iterations"], "min_max_lineno": {"min_lineno": 39, "max_lineno": 43}}, "fit": {"doc": {"long_description": "If ``fit`` has already been called in the past on different training data,\nthis method fits it **again from scratch** using currently set training data.\n\nOn the other hand, caller can call ``fit`` multiple times on the same training data\nto continue fitting.\n\nIf ``fit`` fully fits using provided training data, there is no point in making further\ncalls to this method with same training data, and in fact further calls can be noops,\nor a primitive can decide to refit from scratch.\n\nIn the case fitting can continue with same training data (even if it is maybe not reasonable,\nbecause the internal metric primitive is using looks like fitting will be degrading), if ``fit``\nis called again (without setting training data), the primitive has to continue fitting.\n\nCaller can provide ``timeout`` information to guide the length of the fitting process.\nIdeally, a primitive should adapt its fitting process to try to do the best fitting possible\ninside the time allocated. If this is not possible and the primitive reaches the timeout\nbefore fitting, it should raise a ``TimeoutError`` exception to signal that fitting was\nunsuccessful in the given time. The state of the primitive after the exception should be\nas the method call has never happened and primitive should continue to operate normally.\nThe purpose of ``timeout`` is to give opportunity to a primitive to cleanly manage\nits state instead of interrupting execution from outside. Maintaining stable internal state\nshould have precedence over respecting the ``timeout`` (caller can terminate the misbehaving\nprimitive from outside anyway). If a longer ``timeout`` would produce different fitting,\nthen ``get_call_metadata``'s ``has_finished`` should be set to ``False``.\n\nSome primitives have internal fitting iterations (for example, epochs). For those, caller\ncan provide how many of primitive's internal iterations should a primitive do before returning.\nPrimitives should make iterations as small as reasonable. If ``iterations`` is ``None``,\nthen there is no limit on how many iterations the primitive should do and primitive should\nchoose the best amount of iterations on its own (potentially controlled through\nhyper-parameters). If ``iterations`` is a number, a primitive has to do those number of\niterations (even if not reasonable), if possible. ``timeout`` should still be respected\nand potentially less iterations can be done because of that. Primitives with internal\niterations should make ``get_call_metadata`` returns correct values.\n\nFor primitives which do not have internal iterations, any value of ``iterations``\nmeans that they should fit fully, respecting only ``timeout``.\n\nSubclasses can extend arguments of this method with explicit typed keyword arguments used during\nthe fitting process. For example, they can accept other primitives through an argument representing\na regularizer to use during fitting. The reason why those are not part of constructor arguments is\nthat one can create primitives in any order before having to invoke them or pass them to other\nprimitives.", "short_description": "Fits primitive using inputs and outputs (if any) using currently set training data.", "args": {"timeout": {"description": "A maximum time this primitive should be fitting during this method call, in seconds.", "type_name": "float", "is_optional": false}, "iterations": {"description": "How many of internal iterations should the primitive do.", "type_name": "int", "is_optional": false}}}, "args": ["self", "timeout", "iterations"], "min_max_lineno": {"min_lineno": 44, "max_lineno": 48}}, "get_params": {"doc": {"long_description": "Parameters are all parameters of the primitive which can potentially change during a life-time of\na primitive. Parameters which cannot are passed through constructor.\n\nParameters should include all data which is necessary to create a new instance of this primitive\nbehaving exactly the same as this instance, when the new instance is created by passing the same\nparameters to the class constructor and calling ``set_params``.", "short_description": "Returns parameters of this primitive.", "returns": {"description": "A named tuple of parameters.", "type_name": "Params", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 49, "max_lineno": 53}}, "set_params": {"doc": {"long_description": "Parameters are all parameters of the primitive which can potentially change during a life-time of\na primitive. Parameters which cannot are passed through constructor.", "short_description": "Sets parameters of this primitive.", "args": {"params": {"description": "A named tuple of parameters.", "type_name": "Params", "is_optional": false}}}, "args": ["self", "params"], "min_max_lineno": {"min_lineno": 54, "max_lineno": 58}}, "get_call_metadata": {"doc": {"long_description": "For ``produce``, ``has_finished`` is ``True`` if the last call to ``produce``\nhas produced the final outputs and a call with more time or more iterations\ncannot get different outputs.\n\nFor ``fit``, ``has_finished`` is ``True`` if a primitive has been fully fitted\non current training data and further calls to ``fit`` are unnecessary and will\nnot change anything. ``False`` means that more iterations can be done (but it\ndoes not necessary mean that more iterations are beneficial).\n\nIf a primitive has iterations internally, then ``iterations_done`` contains\nhow many of those iterations have been made during the last call. If primitive\ndoes not support them, ``iterations_done`` is ``None``.\n\nThe reason why this is a separate call is to make return value from ``produce`` and\n``fit`` simpler. Moreover, not all callers might care about this information and for\nmany primitives a default implementation of this method works.", "short_description": "Returns metadata about the last ``produce`` or ``fit`` call.", "returns": {"description": "A named tuple with metadata.", "type_name": "CallMetadata", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 59, "max_lineno": 63}, "calls": ["CallMetadata"]}, "set_random_seed": {"doc": {"long_description": "By default it sets numpy's and Python's random seed.", "short_description": "Sets a random seed for all operations from now on inside the primitive.", "args": {"seed": {"description": "A random seed to use.", "type_name": "int", "is_optional": false}}}, "args": ["self", "seed"], "min_max_lineno": {"min_lineno": 64, "max_lineno": 73}, "calls": ["random.seed", "numpy.random.seed"]}}}, "ContinueFitMixin": {"min_max_lineno": {"min_lineno": 75, "max_lineno": 81}, "methods": {"continue_fit": {"doc": {"long_description": "using currently set training data.\n\nThe difference is what happens when currently set training data is different from\nwhat the primitive might have already been fitted on. ``fit`` fits the primitive from\nscratch, while ``continue_fit`` fits it further and does **not** start from scratch.\n\nCaller can still call ``continue_fit`` multiple times on the same training data as well,\nin which case primitive should try to improve the fit in the same way as with ``fit``.\n\nFrom the perspective of a caller of all other methods, the training data in effect\nis still just currently set training data. If a caller wants to call ``gradient_output``\non all data on which the primitive has been fitted through multiple calls of ``continue_fit``\non different training data, the caller should pass all this data themselves through\nanother call to ``set_training_data``, do not call ``fit`` or ``continue_fit`` again,\nand use ``gradient_output`` method. In this way primitives which truly support\ncontinuation of fitting and need only the latest data to do another fitting, do not\nhave to keep all past training data around themselves.\n\nIf a primitive supports this mixin, then both ``fit`` and ``continue_fit`` can be\ncalled. ``continue_fit`` always continues fitting, if it was started through ``fit``\nor ``continue_fit``. And ``fit`` always restarts fitting, even if previously\n``continue_fit`` was used.\n\nWhen this mixin is supported, then ``get_call_metadata`` method should return\nmetadata also for call of ``continue_fit``.", "short_description": "Similar to base ``fit``, this method fits the primitive using inputs and outputs (if any)", "args": {"timeout": {"description": "A maximum time this primitive should be fitting during this method call, in seconds.", "type_name": "float", "is_optional": false}, "iterations": {"description": "How many of internal iterations should the primitive do.", "type_name": "int", "is_optional": false}}}, "args": ["self", "timeout", "iterations"], "min_max_lineno": {"min_lineno": 77, "max_lineno": 81}}}}, "SamplingCompositionalityMixin": {"doc": {"long_description": "may be likelihood free.\n\nMixin should be used together with the ``PrimitiveBase`` class.", "short_description": "This mixin signals to a caller that the primitive is probabilistic but", "full": "This mixin signals to a caller that the primitive is probabilistic but\nmay be likelihood free.\n\nMixin should be used together with the ``PrimitiveBase`` class."}, "min_max_lineno": {"min_lineno": 83, "max_lineno": 90}, "methods": {"sample": {"doc": {"long_description": "Semantics of ``timeout`` and ``iterations`` is the same as in ``produce``.\n\nWhen this mixin is supported, then ``get_call_metadata`` method should return\nmetadata also for call of ``sample``.", "short_description": "Sample each input from ``inputs`` ``num_samples`` times.", "args": {"inputs": {"description": "The inputs of shape [num_inputs, ...].", "type_name": "Inputs", "is_optional": false}, "num_samples": {"description": "The number of samples to return in a set of samples.", "type_name": "int", "is_optional": false}, "timeout": {"description": "A maximum time this primitive should take to sample outputs during this method call, in seconds.", "type_name": "float", "is_optional": false}, "iterations": {"description": "How many of internal iterations should the primitive do.", "type_name": "int", "is_optional": false}}, "returns": {"description": "The multiple sets of samples of shape [num_samples, num_inputs, ...].", "type_name": "Sequence[Outputs]", "is_generator": false}}, "args": ["self", "inputs", "num_samples", "timeout", "iterations"], "min_max_lineno": {"min_lineno": 86, "max_lineno": 90}}}}, "ProbabilisticCompositionalityMixin": {"doc": {"long_description": "help callers with doing various end-to-end refinements using probabilistic\ncompositionality.\n\nThis mixin adds methods to support at least:\n\n* Metropolis-Hastings\n\nMixin should be used together with the ``PrimitiveBase`` class and ``SamplingCompositionalityMixin`` mixin.", "short_description": "This mixin provides additional abstract methods which primitives should implement to", "full": "This mixin provides additional abstract methods which primitives should implement to\nhelp callers with doing various end-to-end refinements using probabilistic\ncompositionality.\n\nThis mixin adds methods to support at least:\n\n* Metropolis-Hastings\n\nMixin should be used together with the ``PrimitiveBase`` class and ``SamplingCompositionalityMixin`` mixin."}, "min_max_lineno": {"min_lineno": 92, "max_lineno": 99}, "methods": {"log_likelihood": {"doc": {"long_description": "sum_i(log(p(output_i | input_i, params)))", "short_description": "Returns log probability of outputs given inputs and params under this primitive:", "args": {"outputs": {"description": "The outputs.", "type_name": "Outputs", "is_optional": false}, "inputs": {"description": "The inputs.", "type_name": "Inputs", "is_optional": false}}, "returns": {"description": "sum_i(log(p(output_i | input_i, params)))", "type_name": "float", "is_generator": false}}, "args": ["self", "outputs", "inputs"], "min_max_lineno": {"min_lineno": 95, "max_lineno": 99}}}}, "Scores": {"doc": {"long_description": "Their values are of type ``float``.", "short_description": "A type representing a named tuple which holds all the differentiable fields from ``Params``.", "full": "A type representing a named tuple which holds all the differentiable fields from ``Params``.\nTheir values are of type ``float``."}, "min_max_lineno": {"min_lineno": 101, "max_lineno": 103}}, "Gradients": {"doc": {"long_description": "``Optional[float]``. Value is ``None`` if gradient for that part of the structure is not possible.", "short_description": "A type representing a structure of one sample from ``Outputs``, but the values are of type", "full": "A type representing a structure of one sample from ``Outputs``, but the values are of type\n``Optional[float]``. Value is ``None`` if gradient for that part of the structure is not possible."}, "min_max_lineno": {"min_lineno": 105, "max_lineno": 107}}, "GradientCompositionalityMixin": {"doc": {"long_description": "help callers with doing various end-to-end refinements using gradient-based\ncompositionality.\n\nThis mixin adds methods to support at least:\n\n* gradient-based, compositional end-to-end training\n* regularized pre-training\n* multi-task adaptation\n* black box variational inference\n* Hamiltonian Monte Carlo", "short_description": "This mixin provides additional abstract methods which primitives should implement to", "full": "This mixin provides additional abstract methods which primitives should implement to\nhelp callers with doing various end-to-end refinements using gradient-based\ncompositionality.\n\nThis mixin adds methods to support at least:\n\n* gradient-based, compositional end-to-end training\n* regularized pre-training\n* multi-task adaptation\n* black box variational inference\n* Hamiltonian Monte Carlo"}, "min_max_lineno": {"min_lineno": 109, "max_lineno": 126}, "methods": {"gradient_output": {"doc": {"long_description": "When fit term temperature is set to non-zero, it should return the gradient with respect to output of:\n\nsum_i(L(output_i, produce_one(input_i))) + temperature * sum_i(L(training_output_i, produce_one(training_input_i)))\n\nWhen used in combination with the ``ProbabilisticCompositionalityMixin``, it returns gradient\nof sum_i(log(p(output_i | input_i, params))) with respect to output.\n\nWhen fit term temperature is set to non-zero, it should return the gradient with respect to output of:\n\nsum_i(log(p(output_i | input_i, params))) + temperature * sum_i(log(p(training_output_i | training_input_i, params)))", "short_description": "Returns the gradient of loss sum_i(L(output_i, produce_one(input_i))) with respect to output.", "args": {"outputs": {"description": "The outputs.", "type_name": "Outputs", "is_optional": false}, "inputs": {"description": "The inputs.", "type_name": "Inputs", "is_optional": false}}, "returns": {"description": "Gradients.", "type_name": "Gradients[Outputs]", "is_generator": false}}, "args": ["self", "outputs", "inputs"], "min_max_lineno": {"min_lineno": 112, "max_lineno": 116}}, "gradient_params": {"doc": {"long_description": "When fit term temperature is set to non-zero, it should return the gradient with respect to params of:\n\nsum_i(L(output_i, produce_one(input_i))) + temperature * sum_i(L(training_output_i, produce_one(training_input_i)))\n\nWhen used in combination with the ``ProbabilisticCompositionalityMixin``, it returns gradient of\nlog(p(output | input, params)) with respect to params.\n\nWhen fit term temperature is set to non-zero, it should return the gradient with respect to params of:\n\nsum_i(log(p(output_i | input_i, params))) + temperature * sum_i(log(p(training_output_i | training_input_i, params)))", "short_description": "Returns the gradient of loss sum_i(L(output_i, produce_one(input_i))) with respect to params.", "args": {"outputs": {"description": "The outputs.", "type_name": "Outputs", "is_optional": false}, "inputs": {"description": "The inputs.", "type_name": "Inputs", "is_optional": false}}, "returns": {"description": "A named tuple with all fields from ``Params`` and values set to gradient for each parameter.", "type_name": "Scores[Params]", "is_generator": false}}, "args": ["self", "outputs", "inputs"], "min_max_lineno": {"min_lineno": 117, "max_lineno": 121}}, "set_fit_term_temperature": {"doc": {"short_description": "Sets the temperature used in ``gradient_output`` and ``gradient_params``.", "args": {"temperature": {"description": "The temperature to use, [0, inf), typically, [0, 1].", "type_name": "float", "is_optional": false}}}, "args": ["self", "temperature"], "min_max_lineno": {"min_lineno": 122, "max_lineno": 126}}}}, "InspectLossMixin": {"doc": {"long_description": "loss function a primitive is using internally.", "short_description": "Mixin which provides an abstract method for a caller to call to inspect which", "full": "Mixin which provides an abstract method for a caller to call to inspect which\nloss function a primitive is using internally."}, "min_max_lineno": {"min_lineno": 128, "max_lineno": 136}, "methods": {"get_loss_function": {"doc": {"long_description": "a non-standard loss function or if the primitive does not use a loss function at all.", "short_description": "Returns a D3M standard name of the loss function used by the primitive, or ``None`` if using", "returns": {"description": "A D3M standard name of the loss function used.", "type_name": "str", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 132, "max_lineno": 136}}}}}, "body": {"calls": ["TypeVar", "TypeVar", "TypeVar", "NamedTuple"], "store_vars_calls": {"Inputs": "TypeVar", "Outputs": "TypeVar", "Params": "TypeVar", "CallMetadata": "NamedTuple"}}, "is_test": false}