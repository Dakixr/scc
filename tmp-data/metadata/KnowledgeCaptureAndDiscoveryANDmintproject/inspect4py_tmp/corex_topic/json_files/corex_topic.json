{"file": {"path": "/Users/dakixr/Desktop/github/scc/tmp-data/metadata/KnowledgeCaptureAndDiscoveryANDmintproject/P4ML-UI/dsbox-corex/corex_topic/corex_topic.py", "fileNameBase": "corex_topic", "extension": "py", "doc": {"long_description": "Use the principle of Total Cor-relation Explanation (CorEx) to construct hierarchical topic models.\nThis module is specially designed for sparse count data.\n\nGreg Ver Steeg and Aram Galstyan. \"Maximally Informative\nHierarchical Representations of High-Dimensional Data\"\nAISTATS, 2015. arXiv preprint arXiv:1410.7404.\n\nCode below written by:\nGreg Ver Steeg (gregv@isi.edu), 2015.\nRyan J. Gallagher (gallagher.r@husky.neu.edu)\n\nLicense: Apache V2", "short_description": "CorEx Hierarchical Topic Models", "full": "CorEx Hierarchical Topic Models\n\nUse the principle of Total Cor-relation Explanation (CorEx) to construct hierarchical topic models.\nThis module is specially designed for sparse count data.\n\nGreg Ver Steeg and Aram Galstyan. \"Maximally Informative\nHierarchical Representations of High-Dimensional Data\"\nAISTATS, 2015. arXiv preprint arXiv:1410.7404.\n\nCode below written by:\nGreg Ver Steeg (gregv@isi.edu), 2015.\nRyan J. Gallagher (gallagher.r@husky.neu.edu)\n\nLicense: Apache V2"}}, "dependencies": [{"import": "numpy", "alias": "np", "type": "external"}, {"from_module": "os", "import": "makedirs", "type": "external"}, {"from_module": "os", "import": "path", "type": "external"}, {"from_module": "scipy.misc", "import": "logsumexp", "type": "external"}, {"import": "scipy", "alias": "ss", "type": "external"}, {"from_module": "six", "import": "string_types", "type": "external"}], "classes": {"Corex": {"doc": {"long_description": "Code follows sklearn naming/style (e.g. fit(X) to train)", "short_description": "Anchored CorEx hierarchical topic models", "full": "Anchored CorEx hierarchical topic models\nCode follows sklearn naming/style (e.g. fit(X) to train)\n\nParameters\n----------\nn_hidden : int, optional, default=2\n    Number of hidden units.\n\nmax_iter : int, optional\n    Maximum number of iterations before ending.\n\nverbose : int, optional\n    The verbosity level. The default, zero, means silent mode. 1 outputs TC(X;Y) as you go\n    2 output alpha matrix and MIs as you go.\n\ntree : bool, default=True\n    In a tree model, each word can only appear in one topic. tree=False is not yet implemented.\n\ncount : string, {'binarize', 'fraction'}\n    Whether to treat counts (>1) by directly binarizing them, or by constructing a fractional count in [0,1].\n\nseed : integer or numpy.RandomState, optional\n    A random number generator instance to define the state of the\n    random permutations generator. If an integer is given, it fixes the\n    seed. Defaults to the global numpy random number generator.\n\nAttributes\n----------\nlabels : array, [n_samples, n_hidden]\n    Label for each hidden unit for each sample.\n\nclusters : array, [n_visible]\n    Cluster label for each input variable.\n\np_y_given_x : array, [n_samples, n_hidden]\n    p(y_j=1|x) for each sample.\n\nalpha : array-like, shape [n_hidden, n_visible]\n    Adjacency matrix between input variables and hidden units. In range [0,1].\n\nmis : array, [n_hidden, n_visible]\n    Mutual information between each (visible/observed) variable and hidden unit\n\ntcs : array, [n_hidden]\n    TC(X_Gj;Y_j) for each hidden unit\n\ntc : float\n    Convenience variable = Sum_j tcs[j]\n\ntc_history : array\n    Shows value of TC over the course of learning. Hopefully, it is converging.\n    \nwords : list of strings\n    Feature names that label the corresponding columns of X\n\nReferences\n----------\n\n[1]     Greg Ver Steeg and Aram Galstyan. \"Discovering Structure in\n        High-Dimensional Data Through Correlation Explanation.\"\n        NIPS, 2014. arXiv preprint arXiv:1406.1222.\n\n[2]     Greg Ver Steeg and Aram Galstyan. \"Maximally Informative\n        Hierarchical Representations of High-Dimensional Data\"\n        AISTATS, 2015. arXiv preprint arXiv:1410.7404."}, "extend": ["object"], "min_max_lineno": {"min_lineno": 25, "max_lineno": 503}, "methods": {"__init__": {"args": ["self", "n_hidden", "max_iter", "eps", "seed", "verbose", "count", "tree"], "min_max_lineno": {"min_lineno": 95, "max_lineno": 114}, "calls": ["numpy.random.seed", "numpy.set_printoptions", "print", "numpy.seterr", "numpy.seterr"]}, "label": {"doc": {"short_description": "Maximum likelihood labels for some distribution over y's"}, "args": ["self", "p_y_given_x"], "min_max_lineno": {"min_lineno": 115, "max_lineno": 118}}, "labels": {"doc": {"short_description": "Maximum likelihood labels for training data. Can access with self.labels (no parens needed)"}, "args": ["self"], "min_max_lineno": {"min_lineno": 119, "max_lineno": 123}, "calls": ["corex_topic.Corex.label"]}, "clusters": {"doc": {"short_description": "Return cluster labels for variables"}, "args": ["self"], "min_max_lineno": {"min_lineno": 124, "max_lineno": 128}, "calls": ["numpy.argmax"]}, "sign": {"doc": {"short_description": "Return the direction of correlation, positive or negative, for each variable-latent factor."}, "args": ["self"], "min_max_lineno": {"min_lineno": 129, "max_lineno": 133}, "calls": ["numpy.sign"]}, "tc": {"doc": {"short_description": "The total correlation explained by all the Y's."}, "args": ["self"], "min_max_lineno": {"min_lineno": 134, "max_lineno": 139}, "calls": ["numpy.sum"]}, "fit": {"doc": {"short_description": "Fit CorEx on the data X. See fit_transform."}, "args": ["self", "X", "anchors", "anchor_strength", "words"], "returns": [["self"]], "min_max_lineno": {"min_lineno": 140, "max_lineno": 146}, "calls": ["corex_topic.Corex.fit_transform"]}, "fit_transform": {"doc": {"short_description": "Fit CorEx on the data", "args": {"X": {"description": "Count data or some other sparse binary data.", "type_name": "scipy sparse CSR or a numpy matrix, shape = [n_samples, n_visible]", "is_optional": false}, "anchors": {"type_name": "A list of variables anchor each corresponding latent factor to.", "is_optional": false}, "anchor_strength": {"type_name": "How strongly to weight the anchors.", "is_optional": false}, "words": {"type_name": "list of strings that label the corresponding columns of X", "is_optional": false}}, "returns": {"description": "Learned values for each latent factor for each sample.\nY's are sorted so that Y_1 explains most correlation, etc.", "type_name": "array-like, shape = [n_samples, n_hidden]", "is_generator": false, "return_name": "Y"}}, "args": ["self", "X", "anchors", "anchor_strength", "words"], "min_max_lineno": {"min_lineno": 147, "max_lineno": 208}, "calls": ["corex_topic.Corex.preprocess", "corex_topic.Corex.initialize_parameters", "numpy.random.random", "range", "corex_topic.Corex.calculate_latent", "corex_topic.Corex.calculate_mis", "corex_topic.Corex.preprocess_anchors", "enumerate", "corex_topic.Corex.calculate_p_y", "corex_topic.Corex.calculate_theta", "corex_topic.Corex.calculate_latent", "corex_topic.Corex.update_tc", "corex_topic.Corex.print_verbose", "corex_topic.Corex.convergence", "print", "corex_topic.Corex.sort_and_output", "list", "range", "corex_topic.Corex.calculate_alpha", "corex_topic.flatten", "enumerate", "X[].mean", "numpy.argmax"], "store_vars_calls": {"X": "self.preprocess", "p_y_given_x": "np.random.random", "self.mis": "self.calculate_mis", "anchors": "self.preprocess_anchors", "self.log_p_y": "self.calculate_p_y", "self.theta": "self.calculate_theta", "self.alpha": "self.calculate_alpha"}}, "transform": {"doc": {"long_description": "Parameters: samples of data, X, shape = [n_samples, n_visible]\nReturns: , shape = [n_samples, n_hidden]", "short_description": "Label hidden factors for (possibly previously unseen) samples of data."}, "args": ["self", "X", "details"], "returns": [["p_y_given_x", "log_z"], ["p_y_given_x", "log_z"], ["labels"]], "min_max_lineno": {"min_lineno": 209, "max_lineno": 238}, "calls": ["corex_topic.Corex.preprocess", "corex_topic.Corex.calculate_latent", "corex_topic.Corex.label", "numpy.zeros", "range", "numpy.empty", "numpy.einsum", "numpy.einsum", "numpy.einsum", "numpy.einsum", "corex_topic.Corex.dot", "corex_topic.Corex.dot", "numpy.array", "numpy.sum", "range", "numpy.argmax", "range"], "store_vars_calls": {"X": "self.preprocess", "labels": "self.label", "alpha": "np.zeros", "log_p": "np.empty", "c0": "np.einsum", "c1": "np.einsum", "info0": "np.einsum", "info1": "np.einsum"}}, "predict_proba": {"args": ["self", "X"], "min_max_lineno": {"min_lineno": 239, "max_lineno": 241}, "calls": ["corex_topic.Corex.transform"]}, "predict": {"args": ["self", "X"], "min_max_lineno": {"min_lineno": 242, "max_lineno": 244}, "calls": ["corex_topic.Corex.transform"]}, "preprocess": {"doc": {"long_description": "see this variable in a given sample", "short_description": "Data can be binary or can be in the range [0,1], where that is interpreted as the probability to"}, "args": ["self", "X"], "returns": [["X"]], "min_max_lineno": {"min_lineno": 245, "max_lineno": 261}, "calls": ["X.astype.astype.max", "X.astype.astype.astype", "numpy.array().ravel", "numpy.array().ravel().clip", "scipy.diags", "scipy.diags", "numpy.clip", "numpy.array", "numpy.array().ravel", "float", "X.astype.astype.sum", "X.astype.astype.sum", "numpy.array", "X.astype.astype.sum"], "store_vars_calls": {"X": "X.astype", "count": "np.array().ravel", "length": "np.array().ravel().clip", "bg_rate": "ss.diags", "doc_length": "ss.diags", "X.data": "np.clip"}}, "initialize_parameters": {"doc": {"short_description": "Store some statistics about X for future use, and initialize alpha, tc"}, "args": ["self", "X", "words"], "min_max_lineno": {"min_lineno": 262, "max_lineno": 294}, "calls": ["numpy.zeros", "numpy.array().ravel", "numpy.log1p().reshape", "corex_topic.binary_entropy", "numpy.random.random", "numpy.ones", "numpy.any", "numpy.any", "print", "corex_topic.Corex.word_counts.clip", "corex_topic.Corex.word_counts.astype", "print", "numpy.array", "numpy.log1p", "len", "print", "X.sum", "numpy.log1p", "numpy.log", "enumerate", "enumerate"], "store_vars_calls": {"self.tcs": "np.zeros", "self.word_counts": "self.word_counts.clip", "self.lp0": "np.log1p().reshape", "self.h_x": "binary_entropy", "self.alpha": "np.ones"}}, "preprocess_anchors": {"doc": {"short_description": "Preprocess anchors so that it is a list of column indices if not already"}, "args": ["self", "anchors"], "returns": [["anchors"]], "min_max_lineno": {"min_lineno": 295, "max_lineno": 323}, "calls": ["enumerate", "type", "isinstance", "len", "new_anchor_list.append", "NameError", "new_anchor_list.append", "KeyError"]}, "calculate_p_y": {"doc": {"short_description": "Estimate log p(y_j=1)."}, "args": ["self", "p_y_given_x"], "min_max_lineno": {"min_lineno": 324, "max_lineno": 327}, "calls": ["numpy.log", "numpy.mean"]}, "calculate_theta": {"doc": {"short_description": "Estimate marginal parameters from data and expected latent labels."}, "args": ["self", "X", "p_y_given_x", "log_p_y"], "min_max_lineno": {"min_lineno": 328, "max_lineno": 339}, "calls": ["X.T.dot().clip", "corex_topic.log_1mp", "corex_topic.log_1mp", "numpy.array", "corex_topic.log_1mp", "X.T.dot", "numpy.exp", "numpy.exp", "numpy.log", "numpy.log", "numpy.log", "numpy.log"], "store_vars_calls": {"p_dot_y": "X.T.dot().clip", "lp_0g0": "log_1mp", "lp_0g1": "log_1mp"}}, "calculate_alpha": {"doc": {"short_description": "A rule for non-tree CorEx structure."}, "args": ["self", "X", "p_y_given_x", "theta", "log_p_y", "tcs"], "returns": [["alphaopt"]], "min_max_lineno": {"min_lineno": 340, "max_lineno": 367}, "calls": ["corex_topic.Corex.calculate_mis", "numpy.ones", "numpy.sum", "numpy.where", "numpy.max", "numpy.zeros", "numpy.where", "numpy.max", "numpy.errstate", "numpy.exp", "numpy.argsort", "numpy.abs().reshape", "numpy.random.random", "numpy.abs"], "store_vars_calls": {"mis": "self.calculate_mis", "alphaopt": "np.exp", "sa": "np.sum", "self.t": "np.where", "maxmis": "np.max"}}, "calculate_latent": {"doc": {"short_description": "\"Calculate the probability distribution for hidden factors for each sample."}, "args": ["self", "X", "theta"], "min_max_lineno": {"min_lineno": 368, "max_lineno": 379}, "calls": ["numpy.empty", "numpy.einsum", "numpy.einsum", "numpy.einsum", "numpy.einsum", "corex_topic.Corex.normalize_latent", "X.dot", "X.dot", "corex_topic.log_1mp"], "store_vars_calls": {"log_pygx_unnorm": "np.empty", "c0": "np.einsum", "c1": "np.einsum", "info0": "np.einsum", "info1": "np.einsum"}}, "normalize_latent": {"doc": {"long_description": "For each sample in the training set, we estimate a probability distribution\nover y_j, each hidden factor. Here we normalize it. (Eq. 7 in paper.)\nThis normalization factor is used for estimating TC.", "short_description": "Normalize the latent variable distribution", "returns": {"description": "p(y_j|x^l), the probability distribution over all hidden factors,\nfor data samples l = 1...n_samples", "type_name": "3D array, shape (n_hidden, n_samples)", "is_generator": false, "return_name": "p_y_given_x"}}, "args": ["self", "log_pygx_unnorm"], "returns": [["log_pygx", "log_z"]], "min_max_lineno": {"min_lineno": 380, "max_lineno": 406}, "calls": ["numpy.errstate", "scipy.misc.logsumexp", "numpy.exp", "numpy.exp.clip"], "store_vars_calls": {"log_z": "logsumexp", "p_norm": "np.exp"}}, "update_tc": {"args": ["self", "log_z"], "min_max_lineno": {"min_lineno": 407, "max_lineno": 410}, "calls": ["numpy.mean", "corex_topic.Corex.tc_history.append", "numpy.sum"], "store_vars_calls": {"self.tcs": "np.mean"}}, "print_verbose": {"args": ["self"], "min_max_lineno": {"min_lineno": 411, "max_lineno": 417}, "calls": ["print", "print", "print"]}, "convergence": {"args": ["self"], "min_max_lineno": {"min_lineno": 418, "max_lineno": 424}, "calls": ["len", "numpy.mean", "numpy.abs", "numpy.mean"]}, "__getstate__": {"args": ["self"], "returns": [["self_dict"]], "min_max_lineno": {"min_lineno": 425, "max_lineno": 430}, "calls": ["corex_topic.Corex.__dict__.copy"], "store_vars_calls": {"self_dict": "self.__dict__.copy"}}, "save": {"doc": {"short_description": "Pickle a class instance. E.g., corex.save('saved.dat') "}, "args": ["self", "filename"], "min_max_lineno": {"min_lineno": 431, "max_lineno": 447}, "calls": ["pickle.dump", "os.path.dirname", "os.makedirs", "open", "os.path.exists", "os.path.dirname", "os.path.dirname"]}, "sort_and_output": {"args": ["self", "X"], "min_max_lineno": {"min_lineno": 448, "max_lineno": 454}, "calls": ["numpy.argsort"]}, "calculate_mis": {"doc": {"short_description": "Return MI in nats, size n_hidden by n_variables"}, "args": ["self", "theta", "log_p_y"], "min_max_lineno": {"min_lineno": 455, "max_lineno": 460}, "calls": ["numpy.exp().reshape", "numpy.exp", "corex_topic.binary_entropy", "corex_topic.binary_entropy", "numpy.exp", "numpy.exp"], "store_vars_calls": {"p_y": "np.exp().reshape"}}, "get_topics": {"doc": {"long_description": "and each tuple is a pair (word, mutual information). If 'words' was not provided\nto CorEx, then 'word' will be an integer column index of X\n\ntopic_n : integer specifying which topic to get (0-indexed)\nprint_words : True or False, get_topics will attempt to print topics using\n              provided column labels (through 'words') if possible. Otherwise,\n              topics will be consist of column indices of X", "short_description": "Return list of lists of tuples. Each list consists of the top words for a topic"}, "args": ["self", "n_words", "topic", "print_words"], "returns": [["topics"], ["topic"]], "min_max_lineno": {"min_lineno": 461, "max_lineno": 503}, "calls": ["list", "print", "range", "len", "print", "numpy.where", "numpy.argsort", "len", "topics.append"], "store_vars_calls": {"topic_ns": "list"}}}}}, "functions": {"log_1mp": {"args": ["x"], "min_max_lineno": {"min_lineno": 505, "max_lineno": 507}, "calls": ["numpy.log1p", "numpy.exp"]}, "binary_entropy": {"args": ["p"], "min_max_lineno": {"min_lineno": 509, "max_lineno": 511}, "calls": ["numpy.where", "numpy.log2", "numpy.log2", "numpy.np.exp().T", "numpy.np.exp().T"]}, "flatten": {"args": ["a"], "returns": [["b"]], "min_max_lineno": {"min_lineno": 513, "max_lineno": 521}, "calls": ["type", "b.append"]}, "load": {"doc": {"short_description": "Unpickle class instance. "}, "args": ["filename"], "min_max_lineno": {"min_lineno": 523, "max_lineno": 527}, "calls": ["pickle.load", "open"]}}, "is_test": false}