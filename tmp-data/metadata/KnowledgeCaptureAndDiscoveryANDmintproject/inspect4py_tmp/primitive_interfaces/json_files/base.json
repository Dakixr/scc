{"file": {"path": "/Users/dakixr/Desktop/github/scc/tmp-data/metadata/KnowledgeCaptureAndDiscoveryANDmintproject/P4ML-UI/primitive-interfaces/primitive_interfaces/base.py", "fileNameBase": "base", "extension": "py"}, "dependencies": [{"import": "abc", "type": "external"}, {"import": "inspect", "type": "external"}, {"import": "typing", "type": "external"}, {"from_module": "d3m_metadata", "import": "hyperparams", "type": "external"}, {"from_module": "d3m_metadata", "import": "metadata", "alias": "metadata_module", "type": "external"}, {"from_module": "d3m_metadata", "import": "params", "type": "external"}, {"from_module": "d3m_metadata", "import": "problem", "type": "external"}, {"from_module": "d3m_metadata", "import": "types", "type": "external"}, {"from_module": "d3m_metadata", "import": "utils", "type": "external"}], "classes": {"CallResult": {"doc": {"long_description": "(which is different to metadata about the value returned, which is stored\nin ``metadata`` attribute of the value itself).\n\nFor ``produce`` method call, ``has_finished`` is ``True`` if the last call\nto ``produce`` has produced the final outputs and a call with more time or\nmore iterations cannot get different outputs.\n\nFor ``fit`` method call, ``has_finished`` is ``True`` if a primitive has been\nfully fitted on current training data and further calls to ``fit`` are\nunnecessary and will not change anything. ``False`` means that more iterations\ncan be done (but it does not necessary mean that more iterations are beneficial).\n\nIf a primitive has iterations internally, then ``iterations_done`` contains\nhow many of those iterations have been made during the last call. If primitive\ndoes not support them, ``iterations_done`` is ``None``.\n\nThose methods should return value wrapped into this class.", "short_description": "Some methods return additional metadata about the method call itself", "full": "Some methods return additional metadata about the method call itself\n(which is different to metadata about the value returned, which is stored\nin ``metadata`` attribute of the value itself).\n\nFor ``produce`` method call, ``has_finished`` is ``True`` if the last call\nto ``produce`` has produced the final outputs and a call with more time or\nmore iterations cannot get different outputs.\n\nFor ``fit`` method call, ``has_finished`` is ``True`` if a primitive has been\nfully fitted on current training data and further calls to ``fit`` are\nunnecessary and will not change anything. ``False`` means that more iterations\ncan be done (but it does not necessary mean that more iterations are beneficial).\n\nIf a primitive has iterations internally, then ``iterations_done`` contains\nhow many of those iterations have been made during the last call. If primitive\ndoes not support them, ``iterations_done`` is ``None``.\n\nThose methods should return value wrapped into this class.\n\nParameters\n----------\nvalue : Any\n    The value itself of the method call.\nhas_finished : bool\n    Set to ``True`` if it is not reasonable to call the method again anymore.\niterations_done : int\n    How many iterations have been done during a method call, if any."}, "min_max_lineno": {"min_lineno": 25, "max_lineno": 60}, "methods": {"__init__": {"args": ["self", "value", "has_finished", "iterations_done"], "min_max_lineno": {"min_lineno": 56, "max_lineno": 60}}}}, "PrimitiveBaseMeta": {"doc": {"long_description": "metadata can be automatically generated.", "short_description": "A metaclass which provides the primitive instance to metadata so that primitive", "full": "A metaclass which provides the primitive instance to metadata so that primitive\nmetadata can be automatically generated."}, "extend": ["typing.GenericMeta"], "min_max_lineno": {"min_lineno": 62, "max_lineno": 80}, "methods": {"__new__": {"args": ["mcls", "class_name", "bases", "namespace"], "returns": [["cls"], ["cls"]], "min_max_lineno": {"min_lineno": 68, "max_lineno": 80}, "calls": ["typing.GenericMeta.__new__", "inspect.isabstract", "super().__new__.metadata.contribute_to_class", "isinstance", "TypeError"], "store_vars_calls": {"cls": "super().__new__"}}}}, "PrimitiveBase": {"doc": {"long_description": "Class is parametrized using four type variables, ``Inputs``, ``Outputs``, ``Params``,\nand ``Hyperparams``.\n\n``Params`` has to be a subclass of `d3m_metadata.params.Params` and should define\nall fields and their types for parameters which the primitive is fitting.\n\n``Hyperparams`` has to be a subclass of a `d3m_metadata.hyperparams.Hyperparams`.\nHyper-parameters are those primitive's parameters which primitive is not fitting and\ngenerally do not change during a life-time of a primitive.\n\n``Params`` and ``Hyperparams`` have to be pickable and copyable. See `pickle`,\n`copy`, and `copyreg` Python modules for more information.\n\nIn this context we use term method arguments to mean both formal parameters and\nactual parameters of a method. We do this to not confuse method parameters with\nprimitive parameters (``Params``).\n\nAll arguments to all methods are keyword-only. No ``*args`` or ``**kwargs`` should\never be used in any method.\n\nStandardized interface use few public attributes and no other public attributes are\nallowed to assure future compatibility. For your attributes use the convention that\nprivate symbols should start with ``_``.\n\nPrimitives can have methods which are not part of standardized interface classes:\n\n* Additional \"produce\" methods which are prefixed with ``produce_`` and have\n  the same semantics as ``produce`` but potentially return different output\n  container types instead of ``Outputs`` (in such primitive ``Outputs`` is seen as\n  primary output type, but the primitive also has secondary output types).\n  They should return ``CallResult`` and have ``timeout`` and ``iterations`` arguments.\n* Private methods prefixed with ``_``.\n\nNo other public additional methods are allowed. If this represents a problem for you,\nopen an issue. (The rationale is that for other methods an automatic system will not\nunderstand the semantics of the method.)\n\nMethod arguments which start with ``_`` are seen as private and can be used for arguments\nuseful for debugging and testing, but they should not be used by (or even known to) a\ncaller during normal execution. Such arguments have to be optional (have a default value)\nso that the method can be called without the knowledge of the argument.\n\nAll arguments to all methods together are seen as arguments to the primitive as a whole.\nThey are identified by their names. This means that any argument name must have the same\ntype and semantics across all methods, effectively be the same argument. In addition,\nall hyper-parameters can also be overridden for a method call. To allow for this, methods\ncan accept arguments which match hyper-parameter names. All this is necessary so that callers\ncan have easier time determine what values to pass to arguments and that it is\neasier to describe what all values are inputs to a primitive as a whole (set of all\narguments).\n\nTo recap, subclasses can extend arguments of standard methods with explicit typed keyword\narguments used for the method call, or define new \"produce\" methods with arbitrary explicit\ntyped keyword. There are multiple kinds of such arguments allowed:\n\n* An argument which is overriding a hyper-parameter for the duration of the call.\n  It should match a hyper-parameter in name and type. It should be a required argument\n  (no default value) which the caller has to supply (e.g., or with a default value of a\n  hyper-parameter, or with the same hyper-parameter as it was passed to the constructor,\n  or with some other value).\n* An (additional) input argument of any container type and not necessary of ``Inputs``\n  (in such primitive ``Inputs`` is seen as primary input type, but the primitive also has\n  secondary input types).\n* A primitive. In this case a caller will pass in an instance of a primitive (potentially\n  connecting it to other primitives and fitting it, if necessary).\n* An (additional) value argument which is one of standard data types, but not a container type.\n  In this case a caller will try to satisfy the input by creating part of a pipeline which\n  ends with a singleton primitive and extract the singleton value and pass it without a container.\n  This kind of an argument is **discouraged** and should probably be a hyper-parameter instead\n  (because it is unclear how can a caller determine which value is a reasonable value to pass\n  in an automatic way), but it is defined for completeness and so that existing pipelines can be\n  easier described.\n* A private argument prefixed with ``_`` which is used for debugging and testing.\n  It should not be used by (or even known to) a caller during normal execution.\n  Such argument has to be optional (have a default value) so that the method can be called\n  without the knowledge of the argument.\n\nSubclasses of this class allow functional compositionality.", "short_description": "A base class for all TA1 primitives.", "full": "A base class for all TA1 primitives.\n\nClass is parametrized using four type variables, ``Inputs``, ``Outputs``, ``Params``,\nand ``Hyperparams``.\n\n``Params`` has to be a subclass of `d3m_metadata.params.Params` and should define\nall fields and their types for parameters which the primitive is fitting.\n\n``Hyperparams`` has to be a subclass of a `d3m_metadata.hyperparams.Hyperparams`.\nHyper-parameters are those primitive's parameters which primitive is not fitting and\ngenerally do not change during a life-time of a primitive.\n\n``Params`` and ``Hyperparams`` have to be pickable and copyable. See `pickle`,\n`copy`, and `copyreg` Python modules for more information.\n\nIn this context we use term method arguments to mean both formal parameters and\nactual parameters of a method. We do this to not confuse method parameters with\nprimitive parameters (``Params``).\n\nAll arguments to all methods are keyword-only. No ``*args`` or ``**kwargs`` should\never be used in any method.\n\nStandardized interface use few public attributes and no other public attributes are\nallowed to assure future compatibility. For your attributes use the convention that\nprivate symbols should start with ``_``.\n\nPrimitives can have methods which are not part of standardized interface classes:\n\n* Additional \"produce\" methods which are prefixed with ``produce_`` and have\n  the same semantics as ``produce`` but potentially return different output\n  container types instead of ``Outputs`` (in such primitive ``Outputs`` is seen as\n  primary output type, but the primitive also has secondary output types).\n  They should return ``CallResult`` and have ``timeout`` and ``iterations`` arguments.\n* Private methods prefixed with ``_``.\n\nNo other public additional methods are allowed. If this represents a problem for you,\nopen an issue. (The rationale is that for other methods an automatic system will not\nunderstand the semantics of the method.)\n\nMethod arguments which start with ``_`` are seen as private and can be used for arguments\nuseful for debugging and testing, but they should not be used by (or even known to) a\ncaller during normal execution. Such arguments have to be optional (have a default value)\nso that the method can be called without the knowledge of the argument.\n\nAll arguments to all methods together are seen as arguments to the primitive as a whole.\nThey are identified by their names. This means that any argument name must have the same\ntype and semantics across all methods, effectively be the same argument. In addition,\nall hyper-parameters can also be overridden for a method call. To allow for this, methods\ncan accept arguments which match hyper-parameter names. All this is necessary so that callers\ncan have easier time determine what values to pass to arguments and that it is\neasier to describe what all values are inputs to a primitive as a whole (set of all\narguments).\n\nTo recap, subclasses can extend arguments of standard methods with explicit typed keyword\narguments used for the method call, or define new \"produce\" methods with arbitrary explicit\ntyped keyword. There are multiple kinds of such arguments allowed:\n\n* An argument which is overriding a hyper-parameter for the duration of the call.\n  It should match a hyper-parameter in name and type. It should be a required argument\n  (no default value) which the caller has to supply (e.g., or with a default value of a\n  hyper-parameter, or with the same hyper-parameter as it was passed to the constructor,\n  or with some other value).\n* An (additional) input argument of any container type and not necessary of ``Inputs``\n  (in such primitive ``Inputs`` is seen as primary input type, but the primitive also has\n  secondary input types).\n* A primitive. In this case a caller will pass in an instance of a primitive (potentially\n  connecting it to other primitives and fitting it, if necessary).\n* An (additional) value argument which is one of standard data types, but not a container type.\n  In this case a caller will try to satisfy the input by creating part of a pipeline which\n  ends with a singleton primitive and extract the singleton value and pass it without a container.\n  This kind of an argument is **discouraged** and should probably be a hyper-parameter instead\n  (because it is unclear how can a caller determine which value is a reasonable value to pass\n  in an automatic way), but it is defined for completeness and so that existing pipelines can be\n  easier described.\n* A private argument prefixed with ``_`` which is used for debugging and testing.\n  It should not be used by (or even known to) a caller during normal execution.\n  Such argument has to be optional (have a default value) so that the method can be called\n  without the knowledge of the argument.\n\nSubclasses of this class allow functional compositionality.\n\nAttributes\n----------\nmetadata : PrimitiveMetadata\n    Primitive's metadata. Available as a class attribute.\nhyperparams : Hyperparams\n    Hyperparams passed to the constructor.\nrandom_seed : int\n    Random seed passed to the constructor.\ndocker_containers : Dict[str, str]\n    A dict mapping Docker image keys from primitive's metadata to container addresses\n    under which containers are accessible by the primitive."}, "min_max_lineno": {"min_lineno": 82, "max_lineno": 469}, "methods": {"__init__": {"doc": {"long_description": "an instance of type ``Hyperparams``.\n\nMethods can accept per-call overrides for some hyper-parameters (e.g., a threshold\nfor fitting). Those arguments have to match in name and type a hyper-parameter defined\nin ``hyperparams`` object. The value provided in the ``hyperparams`` object serves\nas a default in such case.\n\nProvided random seed should control all randomness used by this primitive.\nPrimitive should behave exactly the same for the same random seed across multiple\ninvocations. You can call `numpy.random.RandomState(random_seed)` to obtain an\ninstance of a random generator using provided seed.\n\nPrimitives can be wrappers around or use one or more Docker images which they can\nspecify as part of  ``installation`` field in their metadata. Each Docker image listed\nthere has a ``key`` field identifying that image. When primitive is created,\n``docker_containers`` contains a mapping between those keys and addresses to which\nprimitive can connect to access a running Docker container for a particular Docker\nimage. Docker containers might be long running and shared between multiple instances\nof a primitive.\n\nNo other arguments to the constructor are allowed (except for private arguments)\nbecause we want instances of primitives to be created without a need for any other\nprior computation.\n\nConstructor should be kept lightweight and not do any computation. No resources\nshould be allocated in the constructor. Resources should be allocated when needed.", "short_description": "All primitives should accept all their hyper-parameters in a constructor as one value,"}, "args": ["self"], "min_max_lineno": {"min_lineno": 186, "max_lineno": 220}}, "produce": {"doc": {"long_description": "The output value should be wrapped inside ``CallResult`` object before returning.\n\nIn many cases producing an output is a quick operation in comparison with ``fit``, but not\nall cases are like that. For example, a primitive can start a potentially long optimization\nprocess to compute outputs. ``timeout`` and ``iterations`` can serve as a way for a caller\nto guide the length of this process.\n\nIdeally, a primitive should adapt its call to try to produce the best outputs possible\ninside the time allocated. If this is not possible and the primitive reaches the timeout\nbefore producing outputs, it should raise a ``TimeoutError`` exception to signal that the\ncall was unsuccessful in the given time. The state of the primitive after the exception\nshould be as the method call has never happened and primitive should continue to operate\nnormally. The purpose of ``timeout`` is to give opportunity to a primitive to cleanly\nmanage its state instead of interrupting execution from outside. Maintaining stable internal\nstate should have precedence over respecting the ``timeout`` (caller can terminate the\nmisbehaving primitive from outside anyway). If a longer ``timeout`` would produce\ndifferent outputs, then ``CallResult``'s ``has_finished`` should be set to ``False``.\n\nSome primitives have internal iterations (for example, optimization iterations).\nFor those, caller can provide how many of primitive's internal iterations\nshould a primitive do before returning outputs. Primitives should make iterations as\nsmall as reasonable. If ``iterations`` is ``None``, then there is no limit on\nhow many iterations the primitive should do and primitive should choose the best amount\nof iterations on its own (potentially controlled through hyper-parameters).\nIf ``iterations`` is a number, a primitive has to do those number of iterations,\nif possible. ``timeout`` should still be respected and potentially less iterations\ncan be done because of that. Primitives with internal iterations should make\n``CallResult`` contain correct values.\n\nFor primitives which do not have internal iterations, any value of ``iterations``\nmeans that they should run fully, respecting only ``timeout``.", "short_description": "Produce primitive's best choice of the output for each of the inputs.", "args": {"inputs": {"description": "The inputs of shape [num_inputs, ...].", "type_name": "Inputs", "is_optional": false}, "timeout": {"description": "A maximum time this primitive should take to produce outputs during this method call, in seconds.", "type_name": "float", "is_optional": false}, "iterations": {"description": "How many of internal iterations should the primitive do.", "type_name": "int", "is_optional": false}}, "returns": {"description": "The outputs of shape [num_inputs, ...] wrapped inside ``CallResult``.", "type_name": "CallResult[Outputs]", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 221, "max_lineno": 224}}, "set_training_data": {"doc": {"long_description": "Standard sublasses in this package do not adhere to the Liskov substitution principle when\ninheriting this method because they do not necessary accept all arguments found in the base\nclass. This means that one has to inspect which arguments are accepted at runtime, or in\nother words, one has to inspect which exactly subclass a primitive implements, if\nyou are accepting a wider range of primitives. This relaxation is allowed only for\nstandard subclasses found in this package. Primitives themselves should not break\nthe Liskov substitution principle but should inherit from a suitable base class.", "short_description": "Sets training data of this primitive.", "args": {"inputs": {"description": "The inputs.", "type_name": "Inputs", "is_optional": false}, "outputs": {"description": "The outputs.", "type_name": "Outputs", "is_optional": false}}}, "args": ["self"], "min_max_lineno": {"min_lineno": 273, "max_lineno": 276}}, "fit": {"doc": {"long_description": "The returned value should be a ``CallResult`` object with ``value`` set to ``None``.\n\nIf ``fit`` has already been called in the past on different training data,\nthis method fits it **again from scratch** using currently set training data.\n\nOn the other hand, caller can call ``fit`` multiple times on the same training data\nto continue fitting.\n\nIf ``fit`` fully fits using provided training data, there is no point in making further\ncalls to this method with same training data, and in fact further calls can be noops,\nor a primitive can decide to refit from scratch.\n\nIn the case fitting can continue with same training data (even if it is maybe not reasonable,\nbecause the internal metric primitive is using looks like fitting will be degrading), if ``fit``\nis called again (without setting training data), the primitive has to continue fitting.\n\nCaller can provide ``timeout`` information to guide the length of the fitting process.\nIdeally, a primitive should adapt its fitting process to try to do the best fitting possible\ninside the time allocated. If this is not possible and the primitive reaches the timeout\nbefore fitting, it should raise a ``TimeoutError`` exception to signal that fitting was\nunsuccessful in the given time. The state of the primitive after the exception should be\nas the method call has never happened and primitive should continue to operate normally.\nThe purpose of ``timeout`` is to give opportunity to a primitive to cleanly manage\nits state instead of interrupting execution from outside. Maintaining stable internal state\nshould have precedence over respecting the ``timeout`` (caller can terminate the misbehaving\nprimitive from outside anyway). If a longer ``timeout`` would produce different fitting,\nthen ``CallResult``'s ``has_finished`` should be set to ``False``.\n\nSome primitives have internal fitting iterations (for example, epochs). For those, caller\ncan provide how many of primitive's internal iterations should a primitive do before returning.\nPrimitives should make iterations as small as reasonable. If ``iterations`` is ``None``,\nthen there is no limit on how many iterations the primitive should do and primitive should\nchoose the best amount of iterations on its own (potentially controlled through\nhyper-parameters). If ``iterations`` is a number, a primitive has to do those number of\niterations (even if not reasonable), if possible. ``timeout`` should still be respected\nand potentially less iterations can be done because of that. Primitives with internal\niterations should make ``CallResult`` contain correct values.\n\nFor primitives which do not have internal iterations, any value of ``iterations``\nmeans that they should fit fully, respecting only ``timeout``.", "short_description": "Fits primitive using inputs and outputs (if any) using currently set training data.", "args": {"timeout": {"description": "A maximum time this primitive should be fitting during this method call, in seconds.", "type_name": "float", "is_optional": false}, "iterations": {"description": "How many of internal iterations should the primitive do.", "type_name": "int", "is_optional": false}}, "returns": {"description": "A ``CallResult`` with ``None`` value.", "type_name": "CallResult[None]", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 294, "max_lineno": 297}}, "get_params": {"doc": {"long_description": "Parameters are all parameters of the primitive which can potentially change during a life-time of\na primitive. Parameters which cannot are passed through constructor.\n\nParameters should include all data which is necessary to create a new instance of this primitive\nbehaving exactly the same as this instance, when the new instance is created by passing the same\nparameters to the class constructor and calling ``set_params``.\n\nNo other arguments to the method are allowed (except for private arguments).", "short_description": "Returns parameters of this primitive.", "returns": {"description": "An instance of parameters.", "type_name": "Params", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 353, "max_lineno": 356}}, "set_params": {"doc": {"long_description": "Parameters are all parameters of the primitive which can potentially change during a life-time of\na primitive. Parameters which cannot are passed through constructor.\n\nNo other arguments to the method are allowed (except for private arguments).", "short_description": "Sets parameters of this primitive.", "args": {"params": {"description": "An instance of parameters.", "type_name": "Params", "is_optional": false}}}, "args": ["self"], "min_max_lineno": {"min_lineno": 373, "max_lineno": 376}}, "can_accept": {"doc": {"long_description": "arguments ``arguments``, if such arguments can be accepted by the method. Otherwise it\nreturns ``None``.\n\nDefault implementation checks structural types of ``arguments`` to match method's arguments' types.\n\nBy (re)implementing this method, a primitive can fine-tune which arguments it accepts\nfor its methods which goes beyond just structural type checking. For example, a primitive might\noperate only on images, so it can accept NumPy arrays, but only those with semantic type\ncorresponding to an image. Or it might check dimensions of an array to assure it operates\non square matrix.", "short_description": "Returns a metadata object describing the output of a call of ``method_name`` method with", "args": {"method_name": {"description": "Name of the method which would be called.", "type_name": "str", "is_optional": false}, "arguments": {"description": "A mapping between argument names and their metadata objects (for pipeline arguments) or types (for other).", "type_name": "Dict[str, Union[Metadata, type]]", "is_optional": false}}, "returns": {"description": "Metadata object of the method call result, or ``None`` if arguments are not accepted\nby the method.", "type_name": "DataMetadata", "is_generator": false}}, "args": ["cls"], "min_max_lineno": {"min_lineno": 389, "max_lineno": 469}, "calls": ["cls.metadata.query", "[].get", "set", "set", "len", "len", "arguments.items", "d3m_metadata.metadata.DataMetadata", "arguments.keys", "primitive_arguments.items", "isinstance", "d3m_metadata.utils.is_subclass", "isinstance", "argument_metadata.query().get", "argument_metadata.query().get", "isinstance", "argument_metadata.query", "argument_metadata.query"], "store_vars_calls": {"metadata": "cls.metadata.query", "method": "[].get", "method_arguments_set": "set", "arguments_keys_set": "set", "argument_type": "argument_metadata.query().get"}}}}, "ContinueFitMixin": {"min_max_lineno": {"min_lineno": 472, "max_lineno": 476}, "methods": {"continue_fit": {"doc": {"long_description": "using currently set training data.\n\nThe difference is what happens when currently set training data is different from\nwhat the primitive might have already been fitted on. ``fit`` fits the primitive from\nscratch, while ``continue_fit`` fits it further and does **not** start from scratch.\n\nCaller can still call ``continue_fit`` multiple times on the same training data as well,\nin which case primitive should try to improve the fit in the same way as with ``fit``.\n\nFrom the perspective of a caller of all other methods, the training data in effect\nis still just currently set training data. If a caller wants to call ``gradient_output``\non all data on which the primitive has been fitted through multiple calls of ``continue_fit``\non different training data, the caller should pass all this data themselves through\nanother call to ``set_training_data``, do not call ``fit`` or ``continue_fit`` again,\nand use ``gradient_output`` method. In this way primitives which truly support\ncontinuation of fitting and need only the latest data to do another fitting, do not\nhave to keep all past training data around themselves.\n\nIf a primitive supports this mixin, then both ``fit`` and ``continue_fit`` can be\ncalled. ``continue_fit`` always continues fitting, if it was started through ``fit``\nor ``continue_fit``. And ``fit`` always restarts fitting, even if previously\n``continue_fit`` was used.", "short_description": "Similar to base ``fit``, this method fits the primitive using inputs and outputs (if any)", "args": {"timeout": {"description": "A maximum time this primitive should be fitting during this method call, in seconds.", "type_name": "float", "is_optional": false}, "iterations": {"description": "How many of internal iterations should the primitive do.", "type_name": "int", "is_optional": false}}, "returns": {"description": "A ``CallResult`` with ``None`` value.", "type_name": "CallResult[None]", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 473, "max_lineno": 476}}}}, "SamplingCompositionalityMixin": {"doc": {"long_description": "may be likelihood free.", "short_description": "This mixin signals to a caller that the primitive is probabilistic but", "full": "This mixin signals to a caller that the primitive is probabilistic but\nmay be likelihood free."}, "min_max_lineno": {"min_lineno": 514, "max_lineno": 523}, "methods": {"sample": {"doc": {"long_description": "Semantics of ``timeout`` and ``iterations`` is the same as in ``produce``.", "short_description": "Sample each input from ``inputs`` ``num_samples`` times.", "args": {"inputs": {"description": "The inputs of shape [num_inputs, ...].", "type_name": "Inputs", "is_optional": false}, "num_samples": {"description": "The number of samples to return in a set of samples.", "type_name": "int", "is_optional": false}, "timeout": {"description": "A maximum time this primitive should take to sample outputs during this method call, in seconds.", "type_name": "float", "is_optional": false}, "iterations": {"description": "How many of internal iterations should the primitive do.", "type_name": "int", "is_optional": false}}, "returns": {"description": "The multiple sets of samples of shape [num_samples, num_inputs, ...] wrapped inside\n``CallResult``. While the output value type is specified as ``Sequence[Outputs]``, the\noutput value can be in fact any container type with dimensions/shape equal to combined\n``Sequence[Outputs]`` dimensions/shape. Subclasses should specify which exactly type\nthe output is.", "type_name": "CallResult[Sequence[Outputs]]", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 520, "max_lineno": 523}}}}, "ProbabilisticCompositionalityMixin": {"doc": {"long_description": "help callers with doing various end-to-end refinements using probabilistic\ncompositionality.\n\nThis mixin adds methods to support at least:\n\n* Metropolis-Hastings\n\nMixin should be used together with ``SamplingCompositionalityMixin`` mixin.", "short_description": "This mixin provides additional abstract methods which primitives should implement to", "full": "This mixin provides additional abstract methods which primitives should implement to\nhelp callers with doing various end-to-end refinements using probabilistic\ncompositionality.\n\nThis mixin adds methods to support at least:\n\n* Metropolis-Hastings\n\nMixin should be used together with ``SamplingCompositionalityMixin`` mixin."}, "min_max_lineno": {"min_lineno": 549, "max_lineno": 619}, "methods": {"log_likelihoods": {"doc": {"long_description": "log(p(output_i | input_i, params))", "short_description": "Returns log probability of outputs given inputs and params under this primitive:", "args": {"outputs": {"description": "The outputs.", "type_name": "Outputs", "is_optional": false}, "inputs": {"description": "The inputs.", "type_name": "Inputs", "is_optional": false}, "timeout": {"description": "A maximum time this primitive should take to produce outputs during this method call, in seconds.", "type_name": "float", "is_optional": false}, "iterations": {"description": "How many of internal iterations should the primitive do.", "type_name": "int", "is_optional": false}}, "returns": {"description": "log(p(output_i | input_i, params))) wrapped inside ``CallResult``.\nWhile the output value type is specified as ``Sequence[float]``, the output\nvalue can be in fact any container type with dimensions/shape equal to\ncombined ``Sequence[float]`` dimensions/shape. Subclasses should specify\nwhich exactly type the output is.", "type_name": "CallResult[Sequence[float]]", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 562, "max_lineno": 565}}, "log_likelihood": {"doc": {"long_description": "sum_i(log(p(output_i | input_i, params)))\n\nBy default it calls ``log_likelihoods`` and computes a sum, but subclasses can\nimplement a more efficient version.", "short_description": "Returns log probability of outputs given inputs and params under this primitive:", "args": {"outputs": {"description": "The outputs.", "type_name": "Outputs", "is_optional": false}, "inputs": {"description": "The inputs.", "type_name": "Inputs", "is_optional": false}, "timeout": {"description": "A maximum time this primitive should take to produce outputs during this method call, in seconds.", "type_name": "float", "is_optional": false}, "iterations": {"description": "How many of internal iterations should the primitive do.", "type_name": "int", "is_optional": false}}, "returns": {"description": "sum_i(log(p(output_i | input_i, params))) wrapped inside ``CallResult``.", "type_name": "CallResult[float]", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 590, "max_lineno": 619}, "calls": ["base.ProbabilisticCompositionalityMixin.log_likelihoods", "base.CallResult", "sum"], "store_vars_calls": {"result": "self.log_likelihoods"}}}}, "Scores": {"doc": {"long_description": "``Params`` but all their values are of type ``float``.", "short_description": "A type representing a version of ``Params`` which holds all the differentiable fields from", "full": "A type representing a version of ``Params`` which holds all the differentiable fields from\n``Params`` but all their values are of type ``float``."}, "min_max_lineno": {"min_lineno": 625, "max_lineno": 627}}, "Gradients": {"doc": {"long_description": "Value is ``None`` if gradient for that part of the structure is not possible.", "short_description": "A type representing a structure similar to ``Container``, but the values are of type ``Optional[float]``.", "full": "A type representing a structure similar to ``Container``, but the values are of type ``Optional[float]``.\nValue is ``None`` if gradient for that part of the structure is not possible."}, "min_max_lineno": {"min_lineno": 639, "max_lineno": 641}}, "GradientCompositionalityMixin": {"doc": {"long_description": "help callers with doing various end-to-end refinements using gradient-based\ncompositionality.\n\nThis mixin adds methods to support at least:\n\n* gradient-based, compositional end-to-end training\n* regularized pre-training\n* multi-task adaptation\n* black box variational inference\n* Hamiltonian Monte Carlo", "short_description": "This mixin provides additional abstract methods which primitives should implement to", "full": "This mixin provides additional abstract methods which primitives should implement to\nhelp callers with doing various end-to-end refinements using gradient-based\ncompositionality.\n\nThis mixin adds methods to support at least:\n\n* gradient-based, compositional end-to-end training\n* regularized pre-training\n* multi-task adaptation\n* black box variational inference\n* Hamiltonian Monte Carlo"}, "min_max_lineno": {"min_lineno": 646, "max_lineno": 775}, "methods": {"gradient_output": {"doc": {"long_description": "When fit term temperature is set to non-zero, it should return the gradient with respect to outputs of:\n\nsum_i(L(output_i, produce_one(input_i))) + temperature * sum_i(L(training_output_i, produce_one(training_input_i)))\n\nWhen used in combination with the ``ProbabilisticCompositionalityMixin``, it returns gradient\nof sum_i(log(p(output_i | input_i, params))) with respect to outputs.\n\nWhen fit term temperature is set to non-zero, it should return the gradient with respect to outputs of:\n\nsum_i(log(p(output_i | input_i, params))) + temperature * sum_i(log(p(training_output_i | training_input_i, params)))", "short_description": "Returns the gradient of loss sum_i(L(output_i, produce_one(input_i))) with respect to outputs.", "args": {"outputs": {"description": "The outputs.", "type_name": "Outputs", "is_optional": false}, "inputs": {"description": "The inputs.", "type_name": "Inputs", "is_optional": false}}, "returns": {"description": "A structure similar to ``Container`` but the values are of type ``Optional[float]``.", "type_name": "Gradients[Outputs]", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 661, "max_lineno": 664}}, "gradient_params": {"doc": {"long_description": "When fit term temperature is set to non-zero, it should return the gradient with respect to params of:\n\nsum_i(L(output_i, produce_one(input_i))) + temperature * sum_i(L(training_output_i, produce_one(training_input_i)))\n\nWhen used in combination with the ``ProbabilisticCompositionalityMixin``, it returns gradient of\nlog(p(output | input, params)) with respect to params.\n\nWhen fit term temperature is set to non-zero, it should return the gradient with respect to params of:\n\nsum_i(log(p(output_i | input_i, params))) + temperature * sum_i(log(p(training_output_i | training_input_i, params)))", "short_description": "Returns the gradient of loss sum_i(L(output_i, produce_one(input_i))) with respect to params.", "args": {"outputs": {"description": "The outputs.", "type_name": "Outputs", "is_optional": false}, "inputs": {"description": "The inputs.", "type_name": "Inputs", "is_optional": false}}, "returns": {"description": "A version of ``Params`` with all differentiable fields from ``Params`` and values set to gradient for each parameter.", "type_name": "Scores[Params]", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 690, "max_lineno": 693}}, "forward": {"doc": {"long_description": "backpropagation-based end-to-end training. Primitive can implement it differently\nthan ``produce``, e.g., forward pass during training can enable dropout layers, or\n``produce`` might not compute gradients while ``forward`` does.\n\nBy default it calls ``produce`` for one iteration.", "short_description": "Similar to ``produce`` method but it is meant to be used for a forward pass during", "args": {"inputs": {"description": "The inputs of shape [num_inputs, ...].", "type_name": "Inputs", "is_optional": false}}, "returns": {"description": "The outputs of shape [num_inputs, ...].", "type_name": "Outputs", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 719, "max_lineno": 740}, "calls": ["base.GradientCompositionalityMixin.produce"]}, "backward": {"doc": {"long_description": "that is being backpropagated end-to-end in a pipeline.\n\nThis is the standard backpropagation algorithm: backpropagation needs to be preceded by a\nforward propagation (``forward`` method).", "short_description": "Returns the gradient with respect to inputs and with respect to params of a loss", "args": {"gradient_outputs": {"description": "The gradient of the loss with respect to this primitive's output. During backpropagation,\nthis comes from the next primitive in the pipeline, i.e., the primitive whose input\nis the output of this primitive during the forward execution with ``forward`` (and ``produce``).", "type_name": "Gradients[Outputs]", "is_optional": false}, "fine_tune": {"description": "If ``True``, executes a fine-tuning gradient descent step as a part of this call.\nThis provides the most straightforward way of end-to-end training/fine-tuning.", "type_name": "bool", "is_optional": false}, "fine_tune_learning_rate": {"description": "Learning rate for end-to-end training/fine-tuning gradient descent steps.", "type_name": "float", "is_optional": false}, "fine_tune_weight_decay": {"description": "L2 regularization (weight decay) coefficient for end-to-end training/fine-tuning gradient\ndescent steps.", "type_name": "float", "is_optional": false}}, "returns": {"description": "A tuple of the gradient with respect to inputs and with respect to params.", "type_name": "Tuple[Gradients[Inputs], Scores[Params]]", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 741, "max_lineno": 745}}, "set_fit_term_temperature": {"doc": {"short_description": "Sets the temperature used in ``gradient_output`` and ``gradient_params``.", "args": {"temperature": {"description": "The temperature to use, [0, inf), typically, [0, 1].", "type_name": "float", "is_optional": false}}}, "args": ["self"], "min_max_lineno": {"min_lineno": 772, "max_lineno": 775}}}}, "LossFunctionMixin": {"doc": {"long_description": "loss function a primitive is using internally, and to compute loss on given\ninputs and outputs.", "short_description": "Mixin which provides abstract methods for a caller to call to inspect which", "full": "Mixin which provides abstract methods for a caller to call to inspect which\nloss function a primitive is using internally, and to compute loss on given\ninputs and outputs."}, "min_max_lineno": {"min_lineno": 784, "max_lineno": 875}, "methods": {"get_loss_function": {"doc": {"long_description": "or ``None`` if using a non-standard loss function or if the primitive does not use a loss\nfunction at all.", "short_description": "Returns a D3M metric value of the loss function used by the primitive during the last fitting,", "returns": {"description": "A D3M standard metric value of the loss function used.", "type_name": "Metric", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 791, "max_lineno": 794}}, "get_loss_primitive": {"doc": {"long_description": "can accept another primitive as a loss function to use, or use it internally. This\nmethod allows a primitive to expose this loss primitive to others, returning directly\nan instance of the primitive being used during the last fitting.", "short_description": "Primitives can be passed to other primitives as arguments. As such, some primitives", "returns": {"description": "A D3M primitive used to compute loss.", "type_name": "PrimitiveBase", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 804, "max_lineno": 807}}, "losses": {"doc": {"long_description": "using a loss function used by the primitive during the last fitting.", "short_description": "Returns the loss L(output_i, produce_one(input_i)) for each (input_i, output_i) pair", "args": {"inputs": {"description": "The inputs.", "type_name": "Inputs", "is_optional": false}, "outputs": {"description": "The outputs.", "type_name": "Outputs", "is_optional": false}, "timeout": {"description": "A maximum time this primitive should take to produce outputs during this method call, in seconds.", "type_name": "float", "is_optional": false}, "iterations": {"description": "How many of internal iterations should the primitive do.", "type_name": "int", "is_optional": false}}, "returns": {"description": "L(output_i, produce_one(input_i)) for each (input_i, output_i) pair\nwrapped inside ``CallResult``.\nWhile the output value type is specified as ``Sequence[float]``, the output\nvalue can be in fact any container type with dimensions/shape equal to\ncombined ``Sequence[float]`` dimensions/shape. Subclasses should specify\nwhich exactly type the output is.", "type_name": "CallResult[Sequence[float]]", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 818, "max_lineno": 821}}, "loss": {"doc": {"long_description": "using a loss function used by the primitive during the last fitting.\n\nBy default it calls ``losses`` and computes a sum, but subclasses can implement\na more efficient version.", "short_description": "Returns the loss sum_i(L(output_i, produce_one(input_i))) for all (input_i, output_i) pairs", "args": {"inputs": {"description": "The inputs.", "type_name": "Inputs", "is_optional": false}, "outputs": {"description": "The outputs.", "type_name": "Outputs", "is_optional": false}, "timeout": {"description": "A maximum time this primitive should take to produce outputs during this method call, in seconds.", "type_name": "float", "is_optional": false}, "iterations": {"description": "How many of internal iterations should the primitive do.", "type_name": "int", "is_optional": false}}, "returns": {"description": "sum_i(L(output_i, produce_one(input_i))) for all (input_i, output_i) pairs\nwrapped inside ``CallResult``.", "type_name": "CallResult[float]", "is_generator": false}}, "args": ["self"], "min_max_lineno": {"min_lineno": 846, "max_lineno": 875}, "calls": ["base.LossFunctionMixin.losses", "base.CallResult", "sum"], "store_vars_calls": {"result": "self.losses"}}}}, "SingletonOutputMixin": {"doc": {"long_description": "its ``produce`` method (and other extra \"produce\" methods) are sequences of\nlength 1. This is useful because a caller can then directly extract this element.\n\nExample of such primitives are primitives which compute loss, which are returning one number\nfor multiple inputs. With this mixin they can return a sequence with this one number, but\ncaller which cares about the loss can extract it out. At the same time, other callers which\noperate only on sequences can continue to operate normally.\n\nWe can see other primitives as mapping primitives, and primitives with this mixin as\nreducing primitives.", "short_description": "If a primitive is inheriting from this mixin, it is signaling that all outputs from", "full": "If a primitive is inheriting from this mixin, it is signaling that all outputs from\nits ``produce`` method (and other extra \"produce\" methods) are sequences of\nlength 1. This is useful because a caller can then directly extract this element.\n\nExample of such primitives are primitives which compute loss, which are returning one number\nfor multiple inputs. With this mixin they can return a sequence with this one number, but\ncaller which cares about the loss can extract it out. At the same time, other callers which\noperate only on sequences can continue to operate normally.\n\nWe can see other primitives as mapping primitives, and primitives with this mixin as\nreducing primitives."}, "min_max_lineno": {"min_lineno": 877, "max_lineno": 879}}}, "body": {"calls": ["typing.TypeVar", "typing.TypeVar", "typing.TypeVar", "typing.TypeVar", "typing.TypeVar", "typing.TypeVar"], "store_vars_calls": {"Inputs": "typing.TypeVar", "Outputs": "typing.TypeVar", "Params": "typing.TypeVar", "Hyperparams": "typing.TypeVar", "T": "typing.TypeVar", "Container": "typing.TypeVar"}}, "is_test": false}