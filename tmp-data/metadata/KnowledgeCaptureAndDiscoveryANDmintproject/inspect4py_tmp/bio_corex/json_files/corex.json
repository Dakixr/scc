{"file": {"path": "/Users/dakixr/Desktop/github/scc/tmp-data/metadata/KnowledgeCaptureAndDiscoveryANDmintproject/P4ML-UI/dsbox-corex/bio_corex/corex.py", "fileNameBase": "corex", "extension": "py", "doc": {"long_description": "Main ideas first described in:\nGreg Ver Steeg and Aram Galstyan. \"Maximally Informative\nHierarchical Representations of High-Dimensional Data\"\nAISTATS, 2015. arXiv preprint(arXiv:1410.7404.)\n\nThe Bayesian smoothing option is described in:\nPepke and Ver Steeg, Comprehensive discovery of subsample gene expression components\nby information explanation: therapeutic implications in cancer. BMC Medical Genomics, 2017.\n\nCode below written by: Greg Ver Steeg (gregv@isi.edu)\n\nLicense: Apache V2", "short_description": "Maximally Informative Representations using CORrelation EXplanation", "full": "Maximally Informative Representations using CORrelation EXplanation\n\nMain ideas first described in:\nGreg Ver Steeg and Aram Galstyan. \"Maximally Informative\nHierarchical Representations of High-Dimensional Data\"\nAISTATS, 2015. arXiv preprint(arXiv:1410.7404.)\n\nThe Bayesian smoothing option is described in:\nPepke and Ver Steeg, Comprehensive discovery of subsample gene expression components\nby information explanation: therapeutic implications in cancer. BMC Medical Genomics, 2017.\n\nCode below written by: Greg Ver Steeg (gregv@isi.edu)\n\nLicense: Apache V2"}}, "dependencies": [{"from_module": "__future__", "import": "print_function", "type": "external"}, {"import": "sys", "type": "external"}, {"import": "numpy", "alias": "np", "type": "external"}, {"from_module": "os", "import": "makedirs", "type": "external"}, {"from_module": "os", "import": "path", "type": "external"}, {"from_module": "numpy", "import": "ma", "type": "external"}, {"from_module": "scipy.misc", "import": "logsumexp", "type": "external"}, {"from_module": "multiprocessing.dummy", "import": "Pool", "type": "external"}], "classes": {"Corex": {"doc": {"long_description": "A method to learn a hierarchy of successively more abstract\nrepresentations of complex data that are maximally\ninformative about the data. This method is unsupervised,\nrequires no assumptions about the data-generating model,\nand scales linearly with the number of variables.\n\nCode follows sklearn naming/style (e.g. fit(X) to train)", "short_description": "Correlation Explanation", "full": "Correlation Explanation\n\nA method to learn a hierarchy of successively more abstract\nrepresentations of complex data that are maximally\ninformative about the data. This method is unsupervised,\nrequires no assumptions about the data-generating model,\nand scales linearly with the number of variables.\n\nCode follows sklearn naming/style (e.g. fit(X) to train)\n\nParameters\n----------\nn_hidden : int, optional, default=2\n    Number of hidden units.\n\ndim_hidden : int, optional, default=2\n    Each hidden unit can take dim_hidden discrete values.\n\nmax_iter : int, optional\n    Maximum number of iterations before ending.\n\nn_repeat : int, optional\n    Repeat several times and take solution with highest TC.\n\nverbose : int, optional\n    The verbosity level. The default, zero, means silent mode. 1 outputs TC(X;Y) as you go\n    2 output alpha matrix and MIs as you go.\n\nseed : integer or numpy.RandomState, optional\n    A random number generator instance to define the state of the\n    random permutations generator. If an integer is given, it fixes the\n    seed. Defaults to the global numpy random number generator.\n\nAttributes\n----------\nlabels : array, [n_hidden, n_samples]\n    Label for each hidden unit for each sample.\n\nclusters : array, [n_visible]\n    Cluster label for each input variable.\n\np_y_given_x : array, [n_hidden, n_samples, dim_hidden]\n    The distribution of latent factors for each sample.\n\nalpha : array-like, shape (n_components,)\n    Adjacency matrix between input variables and hidden units. In range [0,1].\n\nmis : array, [n_hidden, n_visible]\n    Mutual information between each (visible/observed) variable and hidden unit\n\ntcs : array, [n_hidden]\n    TC(X_Gj;Y_j) for each hidden unit\n\ntc : float\n    Convenience variable = Sum_j tcs[j]\n\ntc_history : array\n    Shows value of TC over the course of learning. Hopefully, it is converging.\n\nReferences\n----------\n\n[1]     Greg Ver Steeg and Aram Galstyan. \"Discovering Structure in\n        High-Dimensional Data Through Correlation Explanation.\"\n        NIPS, 2014. arXiv preprint(arXiv:1406.1222.)\n\n[2]     Greg Ver Steeg and Aram Galstyan. \"Maximally Informative\n        Hierarchical Representations of High-Dimensional Data\"\n        AISTATS, 2015. arXiv preprint(arXiv:1410.7404.)\n\n[3]     Pepke and Ver Steeg, Comprehensive discovery of subsample\n        gene expression components by information explanation:\n        therapeutic implications in cancer. BMC Medical Genomics, 2017."}, "extend": ["object"], "min_max_lineno": {"min_lineno": 36, "max_lineno": 579}, "methods": {"__init__": {"args": ["self", "n_hidden", "dim_hidden", "max_iter", "n_repeat", "ram", "max_samples", "n_cpu", "eps", "marginal_description", "smooth_marginals", "missing_values", "seed", "verbose"], "min_max_lineno": {"min_lineno": 113, "max_lineno": 147}, "calls": ["numpy.random.seed", "numpy.set_printoptions", "print", "numpy.seterr", "numpy.seterr", "print"]}, "label": {"doc": {"short_description": "Maximum likelihood labels for some distribution over y's"}, "args": ["self", "p_y_given_x"], "min_max_lineno": {"min_lineno": 148, "max_lineno": 151}, "calls": ["numpy.argmax"]}, "labels": {"doc": {"short_description": "Maximum likelihood labels for training data. Can access with self.labels (no parens needed)"}, "args": ["self"], "min_max_lineno": {"min_lineno": 152, "max_lineno": 156}, "calls": ["corex.Corex.label"]}, "clusters": {"doc": {"short_description": "Return cluster labels for variables"}, "args": ["self"], "min_max_lineno": {"min_lineno": 157, "max_lineno": 161}, "calls": ["numpy.argmax"]}, "tc": {"doc": {"short_description": "The total correlation explained by all the Y's."}, "args": ["self"], "min_max_lineno": {"min_lineno": 162, "max_lineno": 167}, "calls": ["numpy.sum"]}, "fit": {"doc": {"short_description": "Fit CorEx on the data X. See fit_transform."}, "args": ["self", "X"], "returns": [["self"]], "min_max_lineno": {"min_lineno": 168, "max_lineno": 173}, "calls": ["corex.Corex.fit_transform"]}, "fit_transform": {"doc": {"short_description": "Fit CorEx on the data", "args": {"X": {"description": "The data.", "type_name": "array-like, shape = [n_samples, n_visible]", "is_optional": false}}, "returns": {"description": "Learned values for each latent factor for each sample.\nY's are sorted so that Y_1 explains most correlation, etc.", "type_name": "array-like, shape = [n_samples, n_hidden]", "is_generator": false, "return_name": "Y"}}, "args": ["self", "X"], "min_max_lineno": {"min_lineno": 174, "max_lineno": 229}, "calls": ["numpy.ma.masked_equal", "range", "corex.Corex.sort_and_output", "multiprocessing.dummy.Pool", "corex.Corex.initialize_parameters", "range", "print", "corex.Corex.pool.close", "corex.Corex.calculate_p_y", "corex.Corex.calculate_theta", "corex.Corex.calculate_latent", "corex.Corex.update_tc", "corex.Corex.print_verbose", "corex.Corex.convergence", "print", "corex.Corex.__dict__.copy", "corex.Corex.update_alpha"], "store_vars_calls": {"Xm": "ma.masked_equal", "self.pool": "Pool", "self.log_p_y": "self.calculate_p_y", "self.theta": "self.calculate_theta", "best_dict": "self.__dict__.copy"}}, "transform": {"doc": {"long_description": "Parameters: samples of data, X, shape = [n_samples, n_visible]\nReturns: , shape = [n_samples, n_hidden]", "short_description": "Label hidden factors for (possibly previously unseen) samples of data."}, "args": ["self", "X", "details"], "returns": [["p_y_given_x", "log_z"], ["p_y_given_x", "log_z"], ["labels"]], "min_max_lineno": {"min_lineno": 230, "max_lineno": 254}, "calls": ["numpy.ma.masked_equal", "corex.Corex.calculate_latent", "corex.Corex.label", "corex.Corex.calculate_marginals_on_samples", "range", "surprise.append", "numpy.array", "sum", "max", "range", "range"], "store_vars_calls": {"Xm": "ma.masked_equal", "labels": "self.label", "log_marg_x": "self.calculate_marginals_on_samples"}}, "initialize_parameters": {"doc": {"short_description": "Set up starting state", "args": {"X": {"description": "The data.", "type_name": "array-like, shape = [n_samples, n_visible]", "is_optional": false}}}, "args": ["self", "X"], "min_max_lineno": {"min_lineno": 255, "max_lineno": 274}, "calls": ["corex.Corex.initialize_representation", "set", "set", "int", "print", "numpy.unique().tolist", "max", "set", "range", "numpy.unique"]}, "calculate_p_y": {"doc": {"short_description": "Estimate log p(y_j) using a tiny bit of Laplace smoothing to avoid infinities."}, "args": ["self", "p_y_given_x"], "returns": [["log_p_y"]], "min_max_lineno": {"min_lineno": 275, "max_lineno": 280}, "calls": ["numpy.sum", "numpy.log", "numpy.log", "numpy.sum"]}, "calculate_theta": {"doc": {"short_description": "Estimate marginal parameters from data and expected latent labels."}, "args": ["self", "Xm", "p_y_given_x"], "min_max_lineno": {"min_lineno": 281, "max_lineno": 288}, "calls": ["range", "numpy.array", "numpy.logical_not", "theta.append", "corex.Corex.estimate_parameters", "numpy.ma.getmaskarray"], "store_vars_calls": {"not_missing": "np.logical_not"}}, "update_alpha": {"doc": {"short_description": "A rule for non-tree CorEx structure."}, "args": ["self", "p_y_given_x", "theta", "Xm", "tcs"], "min_max_lineno": {"min_lineno": 289, "max_lineno": 310}, "calls": ["numpy.random.choice", "numpy.logical_not", "numpy.empty", "numpy.clip", "range", "numpy.arange", "min", "numpy.ma.getmaskarray", "float", "int", "corex.Corex.calculate_marginals_on_samples", "range", "numpy.where", "numpy.clip", "numpy.argmax", "min", "corex.Corex.unique_info", "numpy.max", "numpy.argmax", "numpy.abs", "numpy.random.random", "numpy.log", "numpy.log"], "store_vars_calls": {"sample": "np.random.choice", "not_missing": "np.logical_not", "alpha": "np.empty", "batch_size": "np.clip", "log_marg_x": "self.calculate_marginals_on_samples", "amax": "np.clip"}}, "unique_info": {"doc": {"long_description": "It indicates whether the ml estimate based on x_i for y_j is correct for sample l\nReturns estimate of fraction of unique info in each predictor j=1...m", "short_description": "*correct* has n_samples rows and n_hidden columns."}, "args": ["self", "correct"], "min_max_lineno": {"min_lineno": 311, "max_lineno": 328}, "calls": ["numpy.clip", "numpy.ones", "numpy.zeros", "numpy.array", "numpy.sum", "numpy.argsort", "numpy.dot", "numpy.logical_and", "numpy.logical_and.astype", "numpy.logical_not", "float", "range"], "store_vars_calls": {"total": "np.clip", "unexplained": "np.logical_and", "unique": "np.zeros"}}, "calculate_latent": {"doc": {"short_description": "\"Calculate the probability distribution for hidden factors for each sample."}, "args": ["self", "theta", "Xm"], "min_max_lineno": {"min_lineno": 329, "max_lineno": 339}, "calls": ["numpy.empty", "numpy.clip", "range", "corex.Corex.normalize_latent", "float", "int", "corex.Corex.calculate_marginals_on_samples", "numpy.einsum"], "store_vars_calls": {"log_p_y_given_x_unnorm": "np.empty", "batch_size": "np.clip", "log_marg_x": "self.calculate_marginals_on_samples"}}, "normalize_latent": {"doc": {"long_description": "For each sample in the training set, we estimate a probability distribution\nover y_j, each hidden factor. Here we normalize it. (Eq. 7 in paper.)\nThis normalization factor is quite useful as described in upcoming work.", "short_description": "Normalize the latent variable distribution", "returns": {"description": "p(y_j|x^l), the probability distribution over all hidden factors,\nfor data samples l = 1...n_samples", "type_name": "3D array, shape (n_hidden, n_samples, dim_hidden)", "is_generator": false, "return_name": "p_y_given_x"}}, "args": ["self", "log_p_y_given_x_unnorm"], "returns": [["log_z"]], "min_max_lineno": {"min_lineno": 340, "max_lineno": 366}, "calls": ["scipy.misc.logsumexp", "log_z.reshape.reshape.reshape", "numpy.exp"], "store_vars_calls": {"log_z": "log_z.reshape"}}, "calculate_p_xi_given_y": {"args": ["self", "xi", "thetai"], "returns": [["z"]], "min_max_lineno": {"min_lineno": 367, "max_lineno": 372}, "calls": ["numpy.logical_not", "numpy.zeros", "corex.Corex.marginal_p", "numpy.ma.getmaskarray", "len"], "store_vars_calls": {"not_missing": "np.logical_not", "z": "np.zeros"}}, "calculate_marginals_on_samples": {"doc": {"long_description": "theta: array parametrizing the marginals\nXm: the data\nreturns log p(y_j|x_i)/p(y_j) for each j,sample,i,y_j. [n_hidden, n_samples, n_visible, dim_hidden]", "short_description": "Calculate the value of the marginal distribution for each variable, for each hidden variable and each sample."}, "args": ["self", "theta", "Xm", "return_ratio"], "returns": [["log_marg_x"]], "min_max_lineno": {"min_lineno": 373, "max_lineno": 398}, "calls": ["numpy.zeros", "zip", "numpy.array().transpose", "range", "corex.Corex.log_p_y.reshape", "corex.Corex.log_p_y.reshape", "corex.Corex.calculate_p_xi_given_y", "len", "numpy.array", "numpy.array", "scipy.misc.logsumexp", "corex.Corex.pool.map", "corex.Corex.pool.map"], "store_vars_calls": {"log_marg_x": "np.array().transpose", "args": "zip"}}, "initialize_representation": {"args": ["self"], "min_max_lineno": {"min_lineno": 399, "max_lineno": 409}, "calls": ["numpy.zeros", "numpy.random.dirichlet", "corex.Corex.normalize_latent", "numpy.ones", "numpy.ones", "numpy.log", "numpy.random.random"], "store_vars_calls": {"self.tcs": "np.zeros", "p_rand": "np.random.dirichlet", "self.alpha": "np.ones"}}, "update_tc": {"args": ["self", "log_z"], "min_max_lineno": {"min_lineno": 410, "max_lineno": 413}, "calls": ["numpy.mean().reshape", "corex.Corex.tc_history.append", "numpy.sum", "numpy.mean"], "store_vars_calls": {"self.tcs": "np.mean().reshape"}}, "print_verbose": {"args": ["self"], "min_max_lineno": {"min_lineno": 414, "max_lineno": 422}, "calls": ["print", "print", "print", "hasattr", "print"]}, "convergence": {"args": ["self"], "min_max_lineno": {"min_lineno": 423, "max_lineno": 429}, "calls": ["len", "numpy.mean", "numpy.abs", "numpy.mean"]}, "__getstate__": {"args": ["self"], "returns": [["self_dict"]], "min_max_lineno": {"min_lineno": 430, "max_lineno": 436}, "calls": ["corex.Corex.__dict__.copy"], "store_vars_calls": {"self_dict": "self.__dict__.copy"}}, "save": {"doc": {"short_description": "Pickle a class instance. E.g., corex.save('saved.dat') "}, "args": ["self", "filename"], "min_max_lineno": {"min_lineno": 437, "max_lineno": 443}, "calls": ["pickle.dump", "os.path.dirname", "os.makedirs", "open", "os.path.exists", "os.path.dirname", "os.path.dirname"]}, "load": {"doc": {"short_description": "Unpickle class instance. E.g., corex = ce.Marginal_Corex().load('saved.dat') "}, "args": ["self", "filename"], "min_max_lineno": {"min_lineno": 444, "max_lineno": 448}, "calls": ["pickle.load", "open"]}, "sort_and_output": {"args": ["self", "Xm"], "min_max_lineno": {"min_lineno": 449, "max_lineno": 464}, "calls": ["corex.Corex.mi_bootstrap", "numpy.argsort", "hasattr", "corex.Corex.calculate_mis"], "store_vars_calls": {"self.mis": "self.calculate_mis"}}, "calculate_mis": {"args": ["self", "p_y_given_x", "theta", "Xm"], "returns": [["mis"]], "min_max_lineno": {"min_lineno": 465, "max_lineno": 477}, "calls": ["numpy.zeros", "numpy.random.choice", "numpy.sum", "numpy.clip", "range", "numpy.arange", "min", "numpy.logical_not", "float", "int", "corex.Corex.calculate_marginals_on_samples", "numpy.ma.getmaskarray", "numpy.einsum"], "store_vars_calls": {"mis": "np.zeros", "sample": "np.random.choice", "n_observed": "np.sum", "batch_size": "np.clip", "log_marg_x": "self.calculate_marginals_on_samples"}}, "mi_bootstrap": {"args": ["self", "Xm", "n_permutation"], "min_max_lineno": {"min_lineno": 478, "max_lineno": 486}, "calls": ["numpy.zeros", "range", "corex.Corex.calculate_theta", "corex.Corex.calculate_mis", "numpy.mean", "numpy.sort", "numpy.random.permutation"], "store_vars_calls": {"mis": "np.zeros", "theta": "self.calculate_theta"}}, "marginal_p": {"doc": {"short_description": "Estimate marginals, log p(xi|yj) for each possible type. "}, "args": ["self", "xi", "thetai"], "min_max_lineno": {"min_lineno": 493, "max_lineno": 512}, "calls": ["xi.reshape.reshape.reshape", "numpy.zeros", "range", "numpy.zeros.transpose", "print", "sys.exit", "len", "numpy.log", "xi.reshape.reshape.reshape"], "store_vars_calls": {"xi": "xi.reshape", "result": "np.zeros"}}, "estimate_parameters": {"args": ["self", "xi", "p_y_given_x"], "min_max_lineno": {"min_lineno": 513, "max_lineno": 556}, "calls": ["numpy.sum().clip", "numpy.einsum", "numpy.einsum", "numpy.array", "numpy.mean", "corex.Corex.estimate_se", "numpy.array", "numpy.mean().reshape", "numpy.sum", "numpy.dot", "p.sum", "numpy.log", "print", "sys.exit", "numpy.sum", "numpy.sum", "numpy.where", "numpy.where", "corex.Corex.estimate_sig", "numpy.where", "len", "numpy.isfinite", "numpy.isfinite", "numpy.arange", "numpy.mean", "numpy.sum", "numpy.isnan", "numpy.where", "numpy.log", "numpy.log"], "store_vars_calls": {"n_obs": "np.sum", "mean0": "np.mean", "prior": "np.mean().reshape", "counts": "np.dot", "G0": "self.estimate_sig", "lam": "np.where"}}, "estimate_se": {"args": ["self", "xi", "p_y_given_x", "n_obs"], "returns": [["m1", "m2", "se1", "se2"]], "min_max_lineno": {"min_lineno": 557, "max_lineno": 569}, "calls": ["numpy.random.choice", "numpy.mean", "numpy.mean", "numpy.sqrt", "numpy.sqrt", "numpy.einsum", "numpy.einsum", "numpy.sum", "numpy.sum", "len", "numpy.random.choice.reshape", "mean_ml.reshape"], "store_vars_calls": {"x_copy": "np.random.choice", "m1": "np.mean", "m2": "np.mean", "se1": "np.sqrt", "se2": "np.sqrt"}}, "estimate_sig": {"args": ["self", "x_select", "p_y_given_x", "n_obs", "prior"], "min_max_lineno": {"min_lineno": 570, "max_lineno": 579}, "calls": ["range", "numpy.mean", "numpy.random.permutation", "numpy.dot", "Gs.append", "numpy.sum", "numpy.where", "numpy.log", "numpy.log"], "store_vars_calls": {"order": "np.random.permutation", "counts": "np.dot"}}}}}, "functions": {"unwrap_f": {"doc": {"short_description": "Multiprocessing pool.map requires a top-level function."}, "args": ["arg"], "min_max_lineno": {"min_lineno": 27, "max_lineno": 30}, "calls": ["corex.Corex.calculate_p_xi_given_y"]}, "logsumexp2": {"doc": {"short_description": "Multiprocessing pool.map requires a top-level function."}, "args": ["z"], "min_max_lineno": {"min_lineno": 31, "max_lineno": 34}, "calls": ["scipy.misc.logsumexp"]}}, "is_test": false}